"use strict";(self.webpackChunkredback_documentation=self.webpackChunkredback_documentation||[]).push([[6991],{88730:(r,e,n)=>{n.r(e),n.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"project-4/Crowd-Monitoring/Crowd-Monitoring-Overview","title":"Crowd-Monitoring-Overview","description":"Last updated by \'01/09/2024\'","source":"@site/docs/project-4/Crowd-Monitoring/Crowd-Monitoring-Overview.md","sourceDirName":"project-4/Crowd-Monitoring","slug":"/project-4/Crowd-Monitoring/Crowd-Monitoring-Overview","permalink":"/redback-documentation/docs/project-4/Crowd-Monitoring/Crowd-Monitoring-Overview","draft":false,"unlisted":false,"editUrl":"https://github.com/Redback-Operations/redback-documentation/blob/main/docs/project-4/Crowd-Monitoring/Crowd-Monitoring-Overview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Crowd Monitoring Overview","permalink":"/redback-documentation/docs/category/crowd-monitoring-overview"},"next":{"title":"MongoDB-Overview","permalink":"/redback-documentation/docs/project-4/Crowd-Monitoring/MongoDB-Overview"}}');var o=n(74848),i=n(28453);const a={sidebar_position:1},s="Crowd Monitoring Overview",c={},l=[{value:"YOLOv8",id:"yolov8",level:2},{value:"Result",id:"result",level:2},{value:"Result",id:"result-1",level:2},{value:"Camo Studio - Virtue Camera on Yolov8",id:"camo-studio---virtue-camera-on-yolov8",level:2},{value:"Result",id:"result-2",level:3},{value:"Visualization",id:"visualization",level:2},{value:"Results",id:"results",level:2}];function d(r){const e={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...r.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Last updated by:"})," SassafrasAU, ",(0,o.jsx)(e.strong,{children:"Last updated on:"})," '01/09/2024'"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Last updated by:"})," SassafrasAU, ",(0,o.jsx)(e.strong,{children:"Last updated on:"})," '01/09/2024'"]}),"\n",(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"crowd-monitoring-overview",children:"Crowd Monitoring Overview"})}),"\n",(0,o.jsxs)(e.p,{children:["The Project Orion aims to apply cutting-edge AI technology to enhance crowd monitoring at various levels. In this semester, we are building on previous efforts to develop an intelligent tracking system, moving from IoT approach to application approach. Our main goal is create a ",(0,o.jsx)(e.strong,{children:"cloud-based"})," computer vision pipeline for improved efficiency and faster processing. Later, the application can connect to a database platform and fetching real-time data."]}),"\n",(0,o.jsx)(e.h2,{id:"yolov8",children:"YOLOv8"}),"\n",(0,o.jsx)(e.p,{children:"Computer Vision (CV) is used in traffic anlysis, automation of manufacturing processed,and human monitoring, which is the essential aspect that we are focusing this semester."}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"YOLOv8"})," is a state of the art to monitor and track people in real-time. By combine that with Supervision library, we can detect and track people."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.img,{alt:"YOLOv8 performance",src:n(87989).A+"",width:"3840",height:"1440"}),"\r\nSource: ",(0,o.jsx)(e.a,{href:"https://docs.ultralytics.com/models/yolov8/",children:"https://docs.ultralytics.com/models/yolov8/"})]}),"\n",(0,o.jsx)(e.h1,{id:"blue-print",children:"Blue print"}),"\n",(0,o.jsx)(e.p,{children:"We are focusing to build a pipeline for real-time camera process."}),"\n",(0,o.jsx)(e.p,{children:"CCTV >> YOLOv8 >> MongoDB >> Website/Dashboard"}),"\n",(0,o.jsx)(e.h1,{id:"initialize-libraries",children:"Initialize libraries"}),"\n",(0,o.jsxs)(e.p,{children:["This tells you the versions of both PyTorch and CUDA that are installed for ",(0,o.jsx)(e.strong,{children:"Environment setup"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:'python\r\nimport torch\r\n!nvcc --version\r\nTORCH_VERSION = ".".join(torch.__version__.split(".")[:2])\r\nCUDA_VERSION = torch.__version__.split("+")[-1]\r\nprint("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)\n'})}),"\n",(0,o.jsx)(e.p,{children:"We will use YOLOv8 in this project:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"python\r\n!pip install ultralytics\r\n\r\nfrom IPython import display\r\ndisplay.clear_output()\r\n\r\nimport ultralytics\r\nultralytics.checks()\n"})}),"\n",(0,o.jsx)(e.p,{children:"Supervision library:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:'python\r\n!pip install supervision==0.2.0\r\n\r\nfrom IPython import display\r\ndisplay.clear_output()\r\n\r\nimport supervision as sv\r\nprint("supervision", sv.__version__)\n'})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"python\r\nimport os\r\nHOME = os.getcwd()\r\nprint(HOME)\n"})}),"\n",(0,o.jsx)(e.h1,{id:"testing-crowd-monitoring",children:"Testing Crowd Monitoring"}),"\n",(0,o.jsx)(e.p,{children:"We will use video from Supervision assets - PEOPLE_WALKING"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.a,{href:"https://media.roboflow.com/supervision/video-examples/people-walking.mp4",children:"https://media.roboflow.com/supervision/video-examples/people-walking.mp4"})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.em,{children:"After downloaded, you will need to import into to your directory if using on Google Colab."})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Video testing with YOLOv8 model"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:'python\r\n# Importing Libraries\r\nfrom ultralytics import YOLO, solutions\r\nfrom ultralytics.solutions import object_counter\r\nimport cv2\r\nimport os\r\nimport numpy as np\r\nfrom IPython.display import display, Image\r\n\r\n# Define the video path - Use your own path\r\nMARKET_SQUARE_VIDEO_PATH = "/content/people-walking.mp4"\r\n\r\n# Open the video file\r\ncap = cv2.VideoCapture(MARKET_SQUARE_VIDEO_PATH)\r\nassert cap.isOpened(), "Error reading video file"\r\n\r\n# Load the YOLO model\r\nmodel = YOLO("yolov8n.pt")\r\n\r\n# Verify the output directory and permissions\r\noutput_dir = "/content"\r\nif not os.path.exists(output_dir):\r\n    os.makedirs(output_dir)\r\n\r\nif not os.access(output_dir, os.W_OK):\r\n    raise PermissionError(f"Write permission denied for the directory {output_dir}")\r\n\r\n# Define the output video path\r\noutput_path = os.path.join(output_dir, "Peoplewalking_v8_29July.mp4")\r\n\r\n# Reading the Video\r\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\r\n\r\n# Initialize VideoWriter with a successful FourCC code\r\nfourcc_code = cv2.VideoWriter_fourcc(*"mp4v")\r\nvideo_writer = cv2.VideoWriter(output_path, fourcc_code, fps, (w, h))\r\nif not video_writer.isOpened():\r\n    raise IOError(f"Error initializing video writer with path {output_path}")\r\n\r\n# Assigning the points for Region of Interest\r\nregion_points = [(20, 1000), (1080, 1000), (1080, 2000), (20, 2000)]\r\n\r\n# Initialize the ObjectCounter with the model\'s class names\r\ncounter = solutions.ObjectCounter(\r\n    view_img=True,\r\n    reg_pts=region_points,\r\n    names=model.names,\r\n    draw_tracks=True,\r\n    line_thickness=2,\r\n)\r\n\r\nwhile cap.isOpened():\r\n    success, im0 = cap.read()\r\n    if not success:\r\n        print("Video frame is empty or video processing has been successfully completed.")\r\n        break\r\n    tracks = model.track(im0, persist=True, show=False, imgsz=1280)\r\n\r\n    im0 = counter.start_counting(im0, tracks)\r\n    video_writer.write(im0)\r\n\r\ncap.release()\r\n\r\nvideo_writer.release()\r\ncv2.destroyAllWindows()\r\nprint(f"Processed video saved to {output_path}")\n'})}),"\n",(0,o.jsx)(e.p,{children:"We have two methods with Camera integration:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"RTSP url - high latency"}),"\n",(0,o.jsx)(e.li,{children:"Virtue Camera - Camo Studio app - low latency"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"RTSP Camera testing with YOLOv8"})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Camera testing with YOLOv8"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"python\r\n!pip install opencv-python numpy ultralytics Flask\n"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:'python\r\n!pip install "pymongo[srv]"\n'})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"python\r\nfrom flask import Flask, Response\r\nimport cv2\r\nimport numpy as np\r\nfrom ultralytics import YOLO\r\nfrom pymongo import MongoClient\r\nfrom datetime import datetime, date\r\nimport time\r\nfrom dotenv import load_dotenv\r\nimport os\r\n\r\n\r\n# Load YOLO model\r\nmodel = YOLO('yolov8n.pt')  # or use a different YOLO version\r\n\r\n# RTSP stream URL\r\n# Retrive the RTSP stream URL from iSpy or Wireshark\r\n# Replace the rtsp_url with your own RTSP stream URL\r\nrtsp_url = ''\r\n\r\n# Connect to the RTSP stream\r\ncap = cv2.VideoCapture(rtsp_url)\r\n\r\n#MongoDB connection\r\nclient = MongoClient('')\r\ndb = client[\"CrowdTracking\"]\r\ncollection = db[\"Crowd\"]\r\n\r\n#variables for frame_id and date format\r\nframe_id = 0\r\ncurrent_date = date.today()\r\nupdate_interval = 1 # Update interval in seconds\r\nlast_update_time = 0\r\n\r\nwhile True:\r\n        current_time = time.time()\r\n        # Read the frame from the stream\r\n        # If the frame was not read, then break the loop and print an error\r\n        ret, frame = cap.read()\r\n        if not ret:\r\n            print('Error reading the frame')\r\n            break\r\n\r\n        # Perform YOLO detection\r\n        results = model(frame)\r\n\r\n        # Process results with box coordinates and confidence scores\r\n        for result in results:\r\n            boxes = result.boxes.cpu().numpy()\r\n            for box in boxes:\r\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\r\n                conf = box.conf[0]\r\n                cls = int(box.cls[0])\r\n            \r\n                if cls == 0:  # Assuming class 0 is person\r\n                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\r\n                    cv2.putText(frame, f'Person: {conf:.2f}', (x1, y1 - 10),\r\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\r\n        \r\n        # This update allows to save the number of persons detected to MongoDB\r\n        # for every update_interval seconds\r\n        if current_time - last_update_time < update_interval:\r\n            now = datetime.now()\r\n            # Save the number of persons detected to MongoDB\r\n            # Save the frame_id, timestamp and the total number of persons detected\r\n            data = {\r\n            \r\n                \"frame_id\": frame_id,\r\n                \"timestamp\": now.strftime(\"%d/%m/%Y %H:%M:%S\"),\r\n                \"total_persons\": len(boxes)\r\n            }\r\n            collection.insert_one(data)\r\n            last_update_time = current_time\r\n\r\n        # Display the number of persons detected on the frame       \r\n        cv2.rectangle(frame, (10, 10), (310, 60), (255, 255, 255), -1)\r\n        cv2.putText(frame, f'Total Persons: {len(boxes)}', (20, 40),\r\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\r\n\r\n        frame_id += 1\r\n\r\n        # Display the frame\r\n        cv2.imshow('Crowd Detection', frame)\r\n        if cv2.waitKey(1) & 0xFF == ord('q'):\r\n            break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"result",children:"Result"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{alt:"Live Camera Tracking",src:n(82430).A+"",width:"3382",height:"2048"})}),"\n",(0,o.jsx)(e.h1,{id:"load-yolo-model",children:"Load YOLO model"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"model = YOLO('yolov8n.pt')  # or use a different YOLO version\r\n\r\n# RTSP stream URL\r\n# Retrive the RTSP stream URL from iSpy or Wireshark\r\n# Replace the rtsp_url with your own RTSP stream URL\r\nrtsp_url = ''\r\n\r\n# Connect to the RTSP stream\r\ncap = cv2.VideoCapture(rtsp_url)\r\n\r\n#MongoDB connection\r\nclient = MongoClient('')\r\ndb = client[\"CrowdTracking\"]\r\ncollection = db[\"Crowd\"]\r\n\r\n#variables for frame_id and date format\r\nframe_id = 0\r\ncurrent_date = date.today()\r\nupdate_interval = 1 # Update interval in seconds\r\nlast_update_time = 0\r\n\r\nwhile True:\r\n        current_time = time.time()\r\n        # Read the frame from the stream\r\n        # If the frame was not read, then break the loop and print an error\r\n        ret, frame = cap.read()\r\n        if not ret:\r\n            print('Error reading the frame')\r\n            break\r\n\r\n        # Perform YOLO detection\r\n        results = model(frame)\r\n\r\n        # Process results with box coordinates and confidence scores\r\n        for result in results:\r\n            boxes = result.boxes.cpu().numpy()\r\n            for box in boxes:\r\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\r\n                conf = box.conf[0]\r\n                cls = int(box.cls[0])\r\n            \r\n                if cls == 0:  # Assuming class 0 is person\r\n                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\r\n                    cv2.putText(frame, f'Person: {conf:.2f}', (x1, y1 - 10),\r\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\r\n        \r\n        # This update allows to save the number of persons detected to MongoDB\r\n        # for every update_interval seconds\r\n        if current_time - last_update_time < update_interval:\r\n            now = datetime.now()\r\n            # Save the number of persons detected to MongoDB\r\n            # Save the frame_id, timestamp and the total number of persons detected\r\n            data = {\r\n            \r\n                \"frame_id\": frame_id,\r\n                \"timestamp\": now.strftime(\"%d/%m/%Y %H:%M:%S\"),\r\n                \"total_persons\": len(boxes)\r\n            }\r\n            collection.insert_one(data)\r\n            last_update_time = current_time\r\n\r\n        # Display the number of persons detected on the frame       \r\n        cv2.rectangle(frame, (10, 10), (310, 60), (255, 255, 255), -1)\r\n        cv2.putText(frame, f'Total Persons: {len(boxes)}', (20, 40),\r\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\r\n\r\n        frame_id += 1\r\n\r\n        # Display the frame\r\n        cv2.imshow('Crowd Detection', frame)\r\n        if cv2.waitKey(1) & 0xFF == ord('q'):\r\n            break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n"})}),"\n",(0,o.jsxs)(e.blockquote,{children:["\n",(0,o.jsx)(e.h2,{id:"result-1",children:"Result"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{alt:"Live Camera Tracking",src:n(82430).A+"",width:"3382",height:"2048"})}),"\n",(0,o.jsx)(e.h2,{id:"camo-studio---virtue-camera-on-yolov8",children:"Camo Studio - Virtue Camera on Yolov8"}),"\n",(0,o.jsx)(e.p,{children:"We will use Camo Studio app. We will need to download it on your mobile device and PC via App store/Google Play. After setup the app via QR code, we can now use our mobile device as a virtue camera for VS Code."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:'python\r\nimport cv2\r\nimport numpy as np\r\nfrom ultralytics import YOLO\r\nfrom collections import defaultdict\r\nfrom utils import calculateHomography, transformPoints\r\nfrom pymongo import MongoClient\r\nimport time as time_module\r\nfrom datetime import datetime\r\n# Load the YOLO model\r\nmodel = YOLO("yolov8n.pt")\r\n\r\n# Connect to the MongoDB database\r\n# and set up data recording\r\nclient = MongoClient("")\r\ndb = client["Crowd_Monitoring"]\r\ncollection = db["Crowd_Count"]\r\n\r\nlastRecorded = time_module.time()\r\n# Connect to the virtue camera using code "1". Code "0" for webcam\r\nrtspUrl = 1\r\ncap = cv2.VideoCapture(rtspUrl)\r\n\r\ntrackHistory = defaultdict(list)\r\n\r\n# Load the floor image\r\nfrom floorReplica import floorReplica\r\ncanvasHeight = 1000\r\ncanvasWidth = 700\r\ntilesX = 25\r\ntilesY = 15\r\nfloorImage = floorReplica(canvasHeight, canvasWidth, tilesX, tilesY, rtspUrl)\r\n\r\nheight, width, channels = floorImage.shape\r\n\r\n# Define the codec and create a VideoWriter object\r\nfourcc = cv2.VideoWriter_fourcc(*\'mp4v\')\r\nvideo = cv2.VideoWriter(\'output.mp4\', fourcc, 20.0, (width, height))\r\n\r\n# Define the source and destination points for the homography matrix\r\n# Calculate the homography matrix\r\nptsSRC = np.array([[28, 1158], [2120, 1112], [1840, 488], [350, 518], [468, 1144]])\r\nptsDST = np.array([[0, 990], [699, 988], [693, 658], [0, 661], [141, 988]])\r\nhomographyMatrix = calculateHomography(ptsSRC, ptsDST)\r\n\r\n# Main loop\r\nwhile True:\r\n#while cap.isOpened():\r\n    success, frame = cap.read()\r\n    results = model.track(frame, persist=True, show=False, imgsz=1280, verbose=True)\r\n    annotatedFrame = floorImage.copy()\r\n    \r\n    # Process camera results with box coordinates and confidence scores\r\n    for result in results:\r\n            boxes_camera = result.boxes.cpu().numpy()\r\n            for box in boxes_camera:\r\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\r\n                conf = box.conf[0]\r\n                cls = int(box.cls[0])\r\n            \r\n                if cls == 0:  # Assuming class 0 is person\r\n                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\r\n                    cv2.putText(frame, f\'Person: {conf:.2f}\', (x1, y1 - 10),\r\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\r\n        \r\n\r\n    # if results[0].boxes is not None and hasattr(results[0].boxes, \'id\'):\r\n    try:\r\n        if results[0].boxes is not None:\r\n            # Check if the boxes attribute contains IDs\r\n            if hasattr(results[0].boxes, \'id\'):\r\n                # Check if there are any detected boxes\r\n                if results[0].boxes.id.numel() > 0:\r\n                    # Convert tensor to NumPy array\r\n                    boxes = results[0].boxes.xywh.cpu().numpy()\r\n                    trackIDs = results[0].boxes.id.cpu().numpy()\r\n                    print(\'Track IDs:\', trackIDs)\r\n                    # Copy floorImage only if objects are detected\r\n                    annotatedFrame = floorImage.copy()\r\n\r\n                    for trackID in np.unique(trackIDs):\r\n                        history = trackHistory[trackID]\r\n                        if len(history) > 1:\r\n                            points = np.array(history, dtype=np.int32)\r\n                            newPoints = transformPoints(points, homographyMatrix)\r\n                            newPoints = newPoints.astype(np.int32)\r\n\r\n                            cv2.polylines(annotatedFrame, [newPoints], isClosed=False, color=(0, 0, 255), thickness=2)\r\n\r\n                    for box, trackID in zip(boxes, trackIDs):\r\n                        x, y, w, h = box\r\n                        center = (int(x), int(y + h / 2))\r\n                        trackHistory[trackID].append(center)\r\n\r\n                        if len(trackHistory[trackID]) > 50:\r\n                            trackHistory[trackID].pop(0)\r\n                    currentTime = time_module.time()\r\n                    print(currentTime)\r\n                    # Record the number of people in the frame every second\r\n                    if currentTime - lastRecorded > 1:\r\n                        frameId = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\r\n                        totalPeople = len(np.unique(trackIDs))\r\n                        print("People", totalPeople)\r\n                        # Convert current time to human-readable format\r\n                        timestamp = time_module.strftime("%d-%m-%Y %H:%M:%S", time_module.localtime(currentTime))\r\n                        print(timestamp)\r\n                        record = {\r\n                                "frameId": frameId,\r\n                                "timestamp": timestamp,\r\n                                "totalPeople": totalPeople\r\n                            }\r\n                        \r\n                        print("Before inserting record into MongoDB")\r\n\r\n                        collection.insert_one(record)\r\n                        print("After inserting record into MongoDB")\r\n                        lastRecorded = currentTime\r\n                    print("People 2", totalPeople)\r\n                    video.write(annotatedFrame)\r\n                else:\r\n                        print("No objects detected. No IDs available.")\r\n            else:\r\n                print("The \'id\' attribute is not present in the boxes.")\r\n        else:\r\n                print("No boxes detected. The \'boxes\' attribute is None.")\r\n    except AttributeError as e:\r\n                print(f"An AttributeError occurred: {e}")\r\n    except Exception as e:\r\n                print(f"An unexpected error occurred: {e}")   \r\n        \r\n    cv2.imshow("Map Tracking", annotatedFrame)\r\n    cv2.imshow("Camera Feed", frame)\r\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n        break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"result-2",children:"Result"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{alt:"Camo demo result",src:n(33486).A+"",width:"768",height:"432"})}),"\n",(0,o.jsx)(e.h2,{id:"visualization",children:"Visualization"}),"\n",(0,o.jsx)(e.p,{children:"We will need to visualize data to display and analysis on dashboard."}),"\n",(0,o.jsxs)(e.blockquote,{children:["\n",(0,o.jsxs)(e.p,{children:["The idea is fetching the tracking path from processed data, plotting their points from ",(0,o.jsx)(e.strong,{children:"camera footage"})," onto ",(0,o.jsx)(e.strong,{children:"2D floor plan"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:["From that, we can do ",(0,o.jsx)(e.strong,{children:"flow analysis"})," and ",(0,o.jsx)(e.strong,{children:"heatmap"})," to evaluate crowd density."]}),"\n",(0,o.jsx)(e.p,{children:"This is a powerful information. It will allow you to easily recognize common pattern of congestion at particular times of day or places. Moreover, it can improve your business performance by arranging staffs and products, make inform decisions to drive sales."}),"\n",(0,o.jsx)(e.p,{children:"Your security camera images are distorted. For example, a one pixel movement at the top of your image corresponds to a much larger movement in the real world than a one pixel movement at the bottom of your image."}),"\n",(0,o.jsxs)(e.blockquote,{children:["\n",(0,o.jsx)(e.p,{children:"Homography Transformation is the solution for camera mapping."}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.img,{alt:"Homography Transformation",src:n(16279).A+"",width:"1244",height:"530"}),"\r\n",(0,o.jsx)(e.img,{alt:"Homography Transformation",src:n(58172).A+"",width:"1234",height:"474"}),"\r\nSource: ",(0,o.jsx)(e.a,{href:"https://zbigatron.com/mapping-camera-coordinates-to-a-2d-floor-plan/",children:"https://zbigatron.com/mapping-camera-coordinates-to-a-2d-floor-plan/"})]}),"\n",(0,o.jsx)(e.p,{children:"We need to calculate corresponse mapping matrix H for homography transformation. We can create the matrix by choosing pixel coordinates in camera view and corresponding pixel coordinates in matching image (at least 4 points)."}),"\n",(0,o.jsxs)(e.blockquote,{children:["\n",(0,o.jsx)(e.p,{children:"Use matrix H to performed track points transformation to plot path on map 2D floor plane."}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{alt:"Matrix Transformation",src:n(41061).A+"",width:"1238",height:"498"})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"TESTING"})}),"\n",(0,o.jsx)(e.p,{children:"Fetching and draw track path of camera view"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:'python\r\nimport cv2\r\nimport numpy as np\r\nfrom ultralytics import YOLO\r\nfrom collections import defaultdict\r\n\r\n# Initialize YOLO model\r\nmodel = YOLO("yolov8n.pt")\r\n\r\n# Open video file\r\nvideo_path = "/content/people-walking.mp4"\r\ncap = cv2.VideoCapture(video_path)\r\n\r\n# Initialize track history\r\ntrack_history = defaultdict(list)\r\n\r\n# Initialize video writer (optional, if you want to save the output)\r\nfourcc = cv2.VideoWriter_fourcc(*"mp4v")\r\nvideo_writer = cv2.VideoWriter(\'/content/tracking_white_output.mp4\', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\r\n\r\n# Assigning the points for Region of Interest\r\nregion_points = [(20, 500), (1080, 500), (1080, 1000), (20, 1000)]\r\n\r\n# Initialize the ObjectCounter with the model\'s class names\r\n# Init Object Counter\r\ncounter = solutions.ObjectCounter(\r\n    view_img=True,\r\n    view_in_counts\t= True,\r\n    view_out_counts\t= True,\r\n    reg_pts=region_points,\r\n    names=model.names,\r\n    #draw_tracks=True,\r\n    line_thickness=2,\r\n)\r\nwhile cap.isOpened():\r\n    success, frame = cap.read()\r\n    if not success:\r\n        break\r\n    # Get the frame dimensions\r\n    height, width, channels = frame.shape\r\n\r\n    # Create a white frame of the same size\r\n    white_frame = np.ones((height, width, channels), dtype=np.uint8) * 255\r\n\r\n    # Track objects in the frame\r\n    results = model.track(frame, persist=True, show=False, imgsz=1280, verbose=True)\r\n\r\n\r\n    # Extract tracking results\r\n    boxes = results[0].boxes.xywh.cpu().numpy()\r\n    track_ids = results[0].boxes.id.int().cpu().numpy()\r\n\r\n    # Draw paths for each track\r\n    #frame_with_counting = counter.start_counting(frame, results)\r\n    #annotated_frame = frame_with_counting.copy()  # Make a copy of the frame to draw paths on\r\n    annotated_frame = white_frame  # Make a copy of the frame to draw paths on\r\n    for track_id in np.unique(track_ids):\r\n        # Get the history for this track_id\r\n        history = track_history[track_id]\r\n        if len(history) > 1:\r\n            points = np.array(history, dtype=np.int32)\r\n            # Draw the path (line connecting the points)\r\n            cv2.polylines(annotated_frame, [points], isClosed=False, color=(0, 255, 0), thickness=2)\r\n\r\n    # Update the track history with new positions\r\n    for box, track_id in zip(boxes, track_ids):\r\n        x, y, w, h = box\r\n        center = (int(x), int(y + h / 2))\r\n        track_history[track_id].append(center)\r\n        # Limit history length\r\n        if len(track_history[track_id]) > 50:\r\n            track_history[track_id].pop(0)\r\n\r\n    # Save or display the frame\r\n    video_writer.write(annotated_frame)\r\n\r\n\r\n# Release resources\r\ncap.release()\r\nvideo_writer.release()\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Draw floor plan"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:'python\r\nfrom google.colab.patches import cv2_imshow\r\nimport numpy as np\r\n\r\ndef create_floor_replica(canvas_height, canvas_width, num_tiles_x, num_tiles_y):\r\n    """\r\n    Create a floor replica with a white canvas and rectangular tiles.\r\n\r\n    Parameters:\r\n    canvas_height (int): Height of the canvas.\r\n    canvas_width (int): Width of the canvas.\r\n    num_tiles_x (int): Number of tiles horizontally.\r\n    num_tiles_y (int): Number of tiles vertically.\r\n\r\n    Returns:\r\n    floor_image (numpy.ndarray): The generated floor image with tiles.\r\n    """\r\n    # Calculate the height of each tile\r\n    tile_height = canvas_height // num_tiles_y\r\n    # Calculate the width of each tile\r\n    tile_width = canvas_width // num_tiles_x\r\n\r\n    # Create a white canvas\r\n    floor_image = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 255\r\n\r\n    # Draw the tiles (rectangles)\r\n    for y in range(0, canvas_height, tile_height):\r\n        for x in range(0, canvas_width, tile_width):\r\n            cv2.rectangle(floor_image, (x, y), (x + tile_width, y + tile_height), (0, 0, 0), 1)\r\n\r\n    return floor_image\r\n\r\n# Example usage\r\nif __name__ == "__main__":\r\n    # Define the canvas size (height and width)\r\n    canvas_height = 1000  # Example height\r\n    canvas_width = 700  # Example width\r\n\r\n    # Number of tiles horizontally and vertically\r\n    num_tiles_x = 25\r\n    num_tiles_y = 15\r\n\r\n    # Create the floor replica\r\n    floor_image = create_floor_replica(canvas_height, canvas_width, num_tiles_x, num_tiles_y)\r\n\r\n    # Display the result\r\n    cv2_imshow(floor_image)\r\n    cv2.imwrite(\'/content/floor_replica.png\', floor_image)\r\n    cv2.waitKey(0)\r\n    cv2.destroyAllWindows()\n'})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Perform transformation and draw tracking path on floor plan"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:'python\r\nimport cv2\r\nimport numpy as np\r\nfrom ultralytics import YOLO\r\nfrom collections import defaultdict\r\n\r\n# Initialize YOLO model\r\nmodel = YOLO("yolov8n.pt")\r\n\r\n# Open video file\r\nvideo_path = "/content/people-walking.mp4"\r\ncap = cv2.VideoCapture(video_path)\r\n\r\n# Initialize track history\r\ntrack_history = defaultdict(list)\r\n\r\n# Load the base image\r\nbase_image_path = \'/content/floor_replica.png\'\r\nfloor_frame = cv2.imread(base_image_path)\r\n\r\n# Ensure the base image is loaded\r\nif floor_frame is None:\r\n    raise ValueError(f"Could not load the base image from {base_image_path}")\r\n\r\n# Get the dimensions of the base image\r\nheight, width, channels = floor_frame.shape\r\n\r\n# Initialize video writer with the dimensions of the base image\r\nfourcc = cv2.VideoWriter_fourcc(*"mp4v")\r\nvideo_writer = cv2.VideoWriter(\'/content/2D_map_output.mp4\', fourcc, 20.0, (width, height))\r\n\r\n# Matching points from 2 views\r\n# Provide points from image 1\r\npts_src = np.array([[28, 1158], [2120, 1112], [1840, 488], [350, 518], [468, 1144]])\r\n# Corresponding points from image 2\r\npts_dst = np.array([[0, 990], [699, 988], [693, 658], [0, 661], [141, 988]])\r\n\r\n# Define homography functions\r\ndef calculate_homography(pts_src, pts_dst):\r\n    return cv2.findHomography(pts_src, pts_dst)[0]\r\n\r\ndef transform_points(points, homography_matrix):\r\n    points = np.concatenate([points, np.ones((points.shape[0], 1))], axis=1)  # Add a column of ones for homogenous coordinates\r\n    transformed_points = homography_matrix.dot(points.T).T  # Apply homography\r\n    transformed_points /= transformed_points[:, 2][:, np.newaxis]  # Normalize by the third coordinate\r\n    return transformed_points[:, :2]\r\n\r\n# Calculate the homography matrix once, since pts_src and pts_dst are constant\r\nhomography_matrix = calculate_homography(pts_src, pts_dst)\r\n\r\n# Process each frame\r\nwhile cap.isOpened():\r\n    success, frame = cap.read()\r\n    if not success:\r\n        break\r\n\r\n    # Track objects in the frame\r\n    results = model.track(frame, persist=True, show=False, imgsz=1280, verbose=True)\r\n\r\n    # Extract tracking results\r\n    boxes = results[0].boxes.xywh.cpu().numpy()\r\n    track_ids = results[0].boxes.id.int().cpu().numpy()\r\n\r\n    # Use a fresh copy of the base image\r\n    annotated_frame = floor_frame.copy()\r\n\r\n    for track_id in np.unique(track_ids):\r\n        # Get the history for this track_id\r\n        history = track_history[track_id]\r\n        if len(history) > 1:\r\n            points = np.array(history, dtype=np.int32)\r\n            # Transform the points using the precomputed homography matrix\r\n            new_points = transform_points(points, homography_matrix)\r\n            new_points = new_points.astype(np.int32)\r\n            # Draw the path (line connecting the points)\r\n            cv2.polylines(annotated_frame, [new_points], isClosed=False, color=(0, 255, 0), thickness=2)\r\n\r\n    # Update the track history with new positions\r\n    for box, track_id in zip(boxes, track_ids):\r\n        x, y, w, h = box\r\n        center = (int(x), int(y + h / 2))\r\n        track_history[track_id].append(center)\r\n        # Limit history length\r\n        if len(track_history[track_id]) > 50:\r\n            track_history[track_id].pop(0)\r\n\r\n    # Save the annotated frame to the video\r\n    video_writer.write(annotated_frame)\r\n\r\n# Release resources\r\ncap.release()\r\nvideo_writer.release()\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"results",children:"Results"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{alt:"Transformation result",src:n(68919).A+"",width:"1152",height:"648"})})]})}function p(r={}){const{wrapper:e}={...(0,i.R)(),...r.components};return e?(0,o.jsx)(e,{...r,children:(0,o.jsx)(d,{...r})}):d(r)}},33486:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/demo-ecf4bfbb354124c347c1da7f3aafabb0.gif"},68919:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/gif-99f42379c96dfb20fe528a52a0d13972.gif"},16279:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/image-1-393c61fea90fa9628c8995e3f479d9e9.png"},58172:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/image-2-bf21c11ef9bafc24005a0081a43d4728.png"},41061:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/image-3-432759eec5f3249b070260c8dca5fdb7.png"},87989:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/image-b15e3dcfb08da2f41ee09f38dea0c8ba.png"},82430:(r,e,n)=>{n.d(e,{A:()=>t});const t=n.p+"assets/images/live_camera-da3d806176c64970ab3e053acf147e16.png"},28453:(r,e,n)=>{n.d(e,{R:()=>a,x:()=>s});var t=n(96540);const o={},i=t.createContext(o);function a(r){const e=t.useContext(i);return t.useMemo((function(){return"function"==typeof r?r(e):{...e,...r}}),[e,r])}function s(r){let e;return e=r.disableParentContext?"function"==typeof r.components?r.components(o):r.components||o:a(r.components),t.createElement(i.Provider,{value:e},r.children)}}}]);