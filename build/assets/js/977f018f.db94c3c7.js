"use strict";(self.webpackChunkredback_documentation=self.webpackChunkredback_documentation||[]).push([[947],{16963:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"project-4/Crowd-Monitoring-Detection/LLMs_training_testing","title":"LLMs_training_testing","description":"Last updated by \'22/09/2024\'","source":"@site/docs/project-4/Crowd-Monitoring-Detection/LLMs_training_testing.md","sourceDirName":"project-4/Crowd-Monitoring-Detection","slug":"/project-4/Crowd-Monitoring-Detection/LLMs_training_testing","permalink":"/redback-documentation/docs/project-4/Crowd-Monitoring-Detection/LLMs_training_testing","draft":false,"unlisted":false,"editUrl":"https://github.com/Redback-Operations/redback-documentation/blob/main/docs/project-4/Crowd-Monitoring-Detection/LLMs_training_testing.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Crowd Monitoring Detection Overview","permalink":"/redback-documentation/docs/category/crowd-monitoring-detection-overview"},"next":{"title":"kafka.tutorial","permalink":"/redback-documentation/docs/project-4/Crowd-Monitoring-Detection/kafka.tutorial"}}');var i=r(74848),o=r(28453);const t={sidebar_position:1},l="Crowd monitoring query system",a={},d=[{value:"Overview",id:"overview",level:2},{value:"Key Features",id:"key-features",level:2},{value:"File Structure",id:"file-structure",level:2},{value:"Dependencies",id:"dependencies",level:2},{value:"Minimum System Requirements",id:"minimum-system-requirements",level:2},{value:"API Endpoints",id:"api-endpoints",level:2},{value:"1. <code>/query_occupancy</code> (POST)",id:"1-query_occupancy-post",level:3},{value:"2. <code>/</code> (GET)",id:"2--get",level:3},{value:"3. <code>/health</code> (GET)",id:"3-health-get",level:3},{value:"4. <code>/chat</code> (GET)",id:"4-chat-get",level:3},{value:"How it Works",id:"how-it-works",level:2},{value:"Model Configuration",id:"model-configuration",level:2},{value:"To training your custom models please use the notebook from train_llms/Train_llms.ipynb",id:"to-training-your-custom-models-please-use-the-notebook-from-train_llmstrain_llmsipynb",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Last updated by:"})," Sumit, ",(0,i.jsx)(n.strong,{children:"Last updated on:"})," '22/09/2024'"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Last updated by:"})," Sumit, ",(0,i.jsx)(n.strong,{children:"Last updated on:"})," '22/09/2024'"]}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"crowd-monitoring-query-system",children:"Crowd monitoring query system"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.strong,{children:"Crowd monitoring query system"})," is a FastAPI-based application that allows users to query room occupancy records based on specific criteria using an instruction-tuned language model. This system integrates a MongoDB database to store and retrieve occupancy data, while leveraging a pre-trained language model to generate natural language responses based on the queried data."]}),"\n",(0,i.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Query Occupancy"}),": Post room occupancy queries using ",(0,i.jsx)(n.code,{children:"time"})," and ",(0,i.jsx)(n.code,{children:"room_id"})," as criteria, and retrieve a natural language response generated by the instruction-tuned language model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language Model"}),": Uses a locally hosted language model (e.g., GPT-2 or others) to generate responses in a human-friendly way."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Database Integration"}),": Connects to a MongoDB collection to fetch and filter occupancy data."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Health Check"}),": API health check to ensure the service is up and running."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Chat Interface"}),": A chat endpoint to test the language model with custom prompts."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"file-structure",children:"File Structure"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"llms.py"})}),": Contains the logic for loading the language model, generating responses, and querying occupancy records."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"main.py"})}),": Implements the FastAPI application, with endpoints for querying occupancy and generating responses from the language model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"mongo_connector.py"})}),": Provides a connection to MongoDB and functionality to query occupancy records based on provided criteria."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:".env"})}),": Holds environment variables such as MongoDB connection URI, database name, collection name, and the language model path."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"dependencies",children:"Dependencies"}),"\n",(0,i.jsx)(n.p,{children:"Ensure you have the following installed:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Python 3.12+"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"FastAPI"})," for building the API"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"pydantic"})," for data validation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"torch"})," for PyTorch"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"transformers"})," for using the pre-trained language models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"pymongo"})," for MongoDB integration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"dotenv"})," for loading environment variables"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"unsloth"})," for loading the models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"langchain"})," for interfacing with databases"]}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Create a Conda environment and activate it"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"conda env create -f environment.yml\r\nconda activate llms\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Configure environment variables"}),":"]}),"\n",(0,i.jsxs)(n.p,{children:["Create a ",(0,i.jsx)(n.code,{children:".env"})," file at the root of the project and add the following variables:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"MONGO_URI=<Your MongoDB URI>\r\nDB=<Your MongoDB Database Name>\r\nCOLLECTION=<Your MongoDB Collection Name>\r\nMODEL=<Path or name of the pre-trained language model>\r\nHF_TOKEN=<Your Hugging Face token if needed>\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Run the FastAPI server"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"fastapi dev main.py\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Configure environment variables"}),":"]}),"\n",(0,i.jsxs)(n.p,{children:["Create a ",(0,i.jsx)(n.code,{children:".env"})," file at the root of the project and add the following variables:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"MONGO_URI=<Your MongoDB URI>\r\nDB=<Your MongoDB Database Name>\r\nCOLLECTION=<Your MongoDB Collection Name>\r\nMODEL=<Path or name of the pre-trained language model>\r\nHF_TOKEN=<Your Hugging Face token if needed> // if you are wants to work with unmodified LLAMA3.1 models\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Important"}),": Ensure you have CUDA installed if you wish to run the model on a GPU."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Here's how you can update the README to include the minimum system requirements for running the API:"}),"\n",(0,i.jsx)(n.h2,{id:"minimum-system-requirements",children:"Minimum System Requirements"}),"\n",(0,i.jsxs)(n.p,{children:["To run the ",(0,i.jsx)(n.strong,{children:"Occupancy Query Service"})," API with the language model, the following hardware specifications are recommended:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CPU"}),": Intel Core i5 or equivalent"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RAM"}),": 16 GB"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU"}),": CUDA-compatible GPU (required for running larger models efficiently)","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"For 3.5B parameter models"}),": Minimum 4 GB VRAM"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"For 8B parameter models"}),": Minimum 6 GB VRAM"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),": If you don't have access to a compatible GPU, the API can still run on CPU, but with significantly slower inference times for larger models.\r\nplease refer to ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu",children:"https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu"})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"api-endpoints",children:"API Endpoints"}),"\n",(0,i.jsxs)(n.h3,{id:"1-query_occupancy-post",children:["1. ",(0,i.jsx)(n.code,{children:"/query_occupancy"})," (POST)"]}),"\n",(0,i.jsx)(n.p,{children:"Query the occupancy database and receive a natural language response."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Request Body"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\r\n    "time": "2023-09-21T15:00:00",\r\n    "room_id": 101\r\n}\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Response"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\r\n    "response": "The occupancy records are [details of the records]."\r\n}\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"2--get",children:["2. ",(0,i.jsx)(n.code,{children:"/"})," (GET)"]}),"\n",(0,i.jsx)(n.p,{children:"Returns a welcome message."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Response"}),":","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\r\n    "message": "Welcome to the occupancy query service!"\r\n}\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"3-health-get",children:["3. ",(0,i.jsx)(n.code,{children:"/health"})," (GET)"]}),"\n",(0,i.jsx)(n.p,{children:"Returns the health status of the API."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Response"}),":","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\r\n    "status": "healthy"\r\n}\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"4-chat-get",children:["4. ",(0,i.jsx)(n.code,{children:"/chat"})," (GET)"]}),"\n",(0,i.jsx)(n.p,{children:"Chat with the language model by passing a custom prompt."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Query Parameter"}),":\r\n",(0,i.jsx)(n.code,{children:"prompt"}),": String prompt to be processed by the model."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Response"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\r\n    "message": "Response from the language model based on the prompt."\r\n}\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"how-it-works",children:"How it Works"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Occupancy Querying"}),": The service accepts a query in the form of ",(0,i.jsx)(n.code,{children:"time"})," and ",(0,i.jsx)(n.code,{children:"room_id"})," (optional). It retrieves matching records from MongoDB and passes the records as a prompt to the language model to generate a human-readable response."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Language Model"}),": A pre-trained language model (loaded using ",(0,i.jsx)(n.code,{children:"FastLanguageModel"}),") is used for inference. It generates a response based on the occupancy records and any additional prompts."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Garbage Collection"}),": To optimize GPU memory usage, garbage collection and GPU memory management are handled using PyTorch\u2019s ",(0,i.jsx)(n.code,{children:"empty_cache"})," and ",(0,i.jsx)(n.code,{children:"ipc_collect"})," methods."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"model-configuration",children:"Model Configuration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quantization"}),": The model is loaded with 4-bit quantization (set by ",(0,i.jsx)(n.code,{children:"load_in_4bit=True"}),") to reduce memory usage."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Max Sequence Length"}),": The maximum sequence length for the model is set to ",(0,i.jsx)(n.code,{children:"2048"}),", and automatic scaling for RoPE (Rotary Positional Embedding) is enabled."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model Customization"}),": You can replace the current model (e.g., GPT-2) with any compatible transformer model by setting the ",(0,i.jsx)(n.code,{children:"MODEL"})," environment variable to the desired model name or path."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"to-training-your-custom-models-please-use-the-notebook-from-train_llmstrain_llmsipynb",children:"To training your custom models please use the notebook from train_llms/Train_llms.ipynb"})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var s=r(96540);const i={},o=s.createContext(i);function t(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);