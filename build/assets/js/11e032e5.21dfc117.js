"use strict";(self.webpackChunkredback_documentation=self.webpackChunkredback_documentation||[]).push([[1407],{98938:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>l,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"data-warehousing/Instructional Documents/Not in Prod","title":"Not in Prod","description":"Last updated by \'09/05/2025\'","source":"@site/docs/data-warehousing/Instructional Documents/Not in Prod.md","sourceDirName":"data-warehousing/Instructional Documents","slug":"/data-warehousing/Instructional Documents/Not in Prod","permalink":"/redback-documentation/docs/data-warehousing/Instructional Documents/Not in Prod","draft":false,"unlisted":false,"editUrl":"https://github.com/Redback-Operations/redback-documentation/blob/main/docs/data-warehousing/Instructional Documents/Not in Prod.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"sidebar_label":"Additional services"},"sidebar":"tutorialSidebar","previous":{"title":"File Upload Service","permalink":"/redback-documentation/docs/data-warehousing/Instructional Documents/How To Access The File Upload Service"},"next":{"title":"GitHub","permalink":"/redback-documentation/docs/data-warehousing/Instructional Documents/GitHub"}}');var s=t(74848),a=t(28453);const o={sidebar_position:6,sidebar_label:"Additional services"},r="Services not running in production",d={},c=[{value:"Nessie",id:"nessie",level:2},{value:"Spark Notebooks and the Virtual Machines",id:"spark-notebooks-and-the-virtual-machines",level:2}];function h(e){const n={a:"a",admonition:"admonition",h1:"h1",h2:"h2",header:"header",img:"img",p:"p",strong:"strong",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Last updated by:"})," RichardWhellum, ",(0,s.jsx)(n.strong,{children:"Last updated on:"})," '09/05/2025'"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Last updated by:"})," RichardWhellum, ",(0,s.jsx)(n.strong,{children:"Last updated on:"})," '09/05/2025'"]}),"\n",(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"services-not-running-in-production",children:"Services not running in production"})}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Document Creation:"})," 22 September, 2024. ",(0,s.jsx)(n.strong,{children:"Last Edited:"})," 29 April, 2025. ",(0,s.jsx)(n.strong,{children:"Authors:"})," kghdxx, Jesse Rees, nouri-devv.\r\n",(0,s.jsx)("br",{})," ",(0,s.jsx)(n.strong,{children:"Document Code:"})," ONB6. ",(0,s.jsx)(n.strong,{children:"Effective Date:"})," 22 September 2024. ",(0,s.jsx)(n.strong,{children:"Expiry Date:"})," 29 April 2026."]})}),"\n",(0,s.jsx)(n.h2,{id:"nessie",children:"Nessie"}),"\n",(0,s.jsx)(n.p,{children:"Nessie is a metadata store that captures information about the files in Dremio and keeps it in case of corruption or for historical analysis."}),"\n",(0,s.jsx)(n.p,{children:"Because the Nessie file in Dremio has the details of each file it can work as a data catalog to quickly sort and find information in a data model which was the original intention to include it in the Data Warehouse stack."}),"\n",(0,s.jsx)(n.p,{children:"The Data Warehouse VM is running a Nessie instance, and the proof of concept has been performed successfully using sample data in Dremio, however at the time of writing there is no Nessie files being stored or utilised in the Data Warehouse Dremio instance."}),"\n",(0,s.jsx)(n.h2,{id:"spark-notebooks-and-the-virtual-machines",children:"Spark Notebooks and the Virtual Machines"}),"\n",(0,s.jsx)(n.p,{children:"The Data Warehouse virtual machine is successfully running Apache Spark as part of the dockerfile."}),"\n",(0,s.jsxs)(n.p,{children:["By following the address: ",(0,s.jsx)(n.a,{href:"http://10.137.0.149:8888/",children:"http://10.137.0.149:8888/"})," this will open a window and start a new Jupyter notebook."]}),"\n",(0,s.jsx)(n.p,{children:"This notebook exists and is running in the virtual machine where Spark jobs can be configured and ran. This represents a functionality to code and run distributed Spark jobs within the virtual machine and has the advantage of being able to process large datasets using the Spark DAG scheduler and partitioning data with distributed computing. At the time of writing without large production datasets in the VM there isn't currently a need for this functionality yet."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"spark",src:t(64841).A+"",width:"1060",height:"528"})})]})}function l(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},64841:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/spark-21038dc1161a89471ce0601af043b1b0.png"},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var i=t(96540);const s={},a=i.createContext(s);function o(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);