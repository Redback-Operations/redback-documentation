"use strict";(self.webpackChunkredback_documentation=self.webpackChunkredback_documentation||[]).push([[9755],{75279:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>h,contentTitle:()=>s,default:()=>l,frontMatter:()=>i,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"data-warehousing/Data Lakehouse/Data Architecture","title":"Data Architecture","description":"Last updated by \'09/05/2025\'","source":"@site/docs/data-warehousing/Data Lakehouse/Data Architecture.md","sourceDirName":"data-warehousing/Data Lakehouse","slug":"/data-warehousing/Data Lakehouse/Data Architecture","permalink":"/redback-documentation/docs/data-warehousing/Data Lakehouse/Data Architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/Redback-Operations/redback-documentation/blob/main/docs/data-warehousing/Data Lakehouse/Data Architecture.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"Data Warehouse Architecture"},"sidebar":"tutorialSidebar","previous":{"title":"Data Warehouse Requirements","permalink":"/redback-documentation/docs/data-warehousing/Data Lakehouse/Data Warehouse Requirements"},"next":{"title":"MongoDB","permalink":"/redback-documentation/docs/category/mongodb"}}');var r=t(74848),o=t(28453);const i={sidebar_position:2,sidebar_label:"Data Warehouse Architecture"},s="Data Architecture",h={},d=[{value:"Background",id:"background",level:2},{value:"Data Lakehouse Decision",id:"data-lakehouse-decision",level:2},{value:"How to use the Data Lakehouse",id:"how-to-use-the-data-lakehouse",level:2},{value:"For users with access to the data Lakehouse",id:"for-users-with-access-to-the-data-lakehouse",level:4},{value:"Storing flat files/structured data requires following the proposed layout below:",id:"storing-flat-filesstructured-data-requires-following-the-proposed-layout-below",level:4},{value:"Using object storage:",id:"using-object-storage",level:4},{value:"Lakehouse Architecture",id:"lakehouse-architecture",level:2},{value:"Modelling Architecture",id:"modelling-architecture",level:2},{value:"Folder Architecture",id:"folder-architecture",level:2},{value:"The Tech Stack",id:"the-tech-stack",level:2},{value:"Data Storage",id:"data-storage",level:3},{value:"Docker",id:"docker",level:3},{value:"MinIO",id:"minio",level:3},{value:"Nessie",id:"nessie",level:3},{value:"Format",id:"format",level:3},{value:"Dremio",id:"dremio",level:3},{value:"Migration",id:"migration",level:3},{value:"Orchestration and Pipelines",id:"orchestration-and-pipelines",level:2},{value:"Mongo DB",id:"mongo-db",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const a={admonition:"admonition",br:"br",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Last updated by:"})," RichardWhellum, ",(0,r.jsx)(a.strong,{children:"Last updated on:"})," '09/05/2025'"]}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Last updated by:"})," RichardWhellum, ",(0,r.jsx)(a.strong,{children:"Last updated on:"})," '09/05/2025'"]}),"\n",(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"data-architecture",children:"Data Architecture"})}),"\n",(0,r.jsx)(a.admonition,{type:"info",children:(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.strong,{children:"Document Creation:"})," 19 May, 2024. ",(0,r.jsx)(a.strong,{children:"Last Edited:"})," 19 May, 2024. ",(0,r.jsx)(a.strong,{children:"Authors:"})," Kaleb, kghdxx.\r\n",(0,r.jsx)("br",{})," ",(0,r.jsx)(a.strong,{children:"Document Code:"})," ARC1. ",(0,r.jsx)(a.strong,{children:"Effective Date:"})," 19 May, 2024. ",(0,r.jsx)(a.strong,{children:"Expiry Date:"})," 19 May, 2025."]})}),"\n",(0,r.jsx)(a.h2,{id:"background",children:"Background"}),"\n",(0,r.jsx)(a.p,{children:"At the end of trimester 3 2023, Reback operations was operating without a data warehouse, the\r\ncompany members were making do with what was available and performing research against\r\nalternative storage methods with limited success. Trimester 1 2024 began with the first\r\ncompanywide requirements gathering in an effort to match a potential data warehouse solution\r\ncorrectly with the needs of the company. This process involved interviews with company\r\nleaders and a survey for company members, resulting in an accurate list of requirements and a\r\nplan for the implementation of a data warehouse solution. Following the requirements gathering\r\nactivities an accompanying options paper was prepared, as well as a strategy for\r\nimplementation as well as a change in direction, this change was to explore the deployment of a\r\nData Lakehouse solution."}),"\n",(0,r.jsx)(a.p,{children:"This decision was presented and discussed and ultimately accepted during Tri 1 2024 panel\r\nmeetings and has the endorsement of the company leaders and Director."}),"\n",(0,r.jsx)(a.p,{children:"This document has two purposes; to explain the architecture of the data Lakehouse and how to\r\nuse/access data in the data Lakehouse."}),"\n",(0,r.jsx)(a.p,{children:"Selecting the Data Lakehouse platform and associated tools required an extensive decision\r\nmaking process to ensure that the most appropriate supporting technologies were chosen for\r\nthe data Lakehouse tool. As a result of this Dremio was chosen as the Data Lakehouse\r\nplatform."}),"\n",(0,r.jsx)(a.h2,{id:"data-lakehouse-decision",children:"Data Lakehouse Decision"}),"\n",(0,r.jsx)(a.p,{children:"Considering multiple options, given the requirements outlined Dremio has been selected as most\r\nappropriate option. Dremio was selected as the Data Lakehouse platform given it meets individual team\r\nrequirements and can also incorporate the Deakin virtual machine as a storage layer."}),"\n",(0,r.jsx)(a.p,{children:"With the combination of other tools Dremio can operate on top of and query data from the VM\u2019s storage.\r\nThis is critical because in the absence of a cloud storage budget all software and storage is effectively\r\nrequired to integrate with the VM as a data source to be considered viable. Unfortunately, this disqualified\r\nIOmete (a cloud alternative to Dremio) and other cloud-only operating systems for this Trimesters Data\r\nLakehouse decision, when in the event of a cloud storage layer option there was a potential advantages\r\nfor these platforms as a use-case."}),"\n",(0,r.jsx)(a.p,{children:"With the requirements given careful thought we can move onto how to access the Data Lakehouse."}),"\n",(0,r.jsx)(a.h2,{id:"how-to-use-the-data-lakehouse",children:"How to use the Data Lakehouse"}),"\n",(0,r.jsx)(a.p,{children:"Firstly, to enter data into the data Lakehouse an admin of the Lakehouse (someone from the data\r\nwarehouse team) needs to set up a Dremio account on your behalf."}),"\n",(0,r.jsx)(a.p,{children:"This effectively gives you access to the Lakehouse, with a user account and password."}),"\n",(0,r.jsx)(a.p,{children:"To keep the data Lakehouse from becoming a data swamp it\u2019s important to have tight governance on how\r\ndata is stored on the Lakehouse."}),"\n",(0,r.jsx)(a.p,{children:"It\u2019s proposed that the best practice for redback is to have one user who contributes data to the Lakehouse\r\nper team unless required so the overarching data warehouse administration can keep track of data\r\nownership."}),"\n",(0,r.jsx)(a.h4,{id:"for-users-with-access-to-the-data-lakehouse",children:"For users with access to the data Lakehouse"}),"\n",(0,r.jsx)(a.p,{children:"There are two forms of data storage:"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsx)(a.li,{children:"Structured data, (csv\u2019s flat files)"}),"\n",(0,r.jsx)(a.li,{children:"Unstructured data (images, videos or streaming data, Json)"}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Depending on what type of data you are storing, depends on what process to follow."}),"\n",(0,r.jsx)(a.h4,{id:"storing-flat-filesstructured-data-requires-following-the-proposed-layout-below",children:"Storing flat files/structured data requires following the proposed layout below:"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsx)(a.li,{children:"Ask your team leader who is the data warehouse representative, they will have access to the Data\r\nLakehouse frontend."}),"\n",(0,r.jsx)(a.li,{children:"The Data Warehouse team member can share the link to Dremio, which when followed opens the\r\nUI in your browser."}),"\n",(0,r.jsx)(a.li,{children:"Find the folder representing the Team/Project that you are a part of and follow the subfolders to\r\nthe appropriate level e.g project_6 > 2024 > t1_2024 > sub-project name > task_name >\r\nstudent_ID(optional)"}),"\n",(0,r.jsx)(a.li,{children:"Use the upload data function in the folder icon on the right-hand side of the UI."}),"\n"]}),"\n",(0,r.jsx)(a.h4,{id:"using-object-storage",children:"Using object storage:"}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsx)(a.li,{children:"Same as steps 1 & 2 above"}),"\n",(0,r.jsx)(a.li,{children:"Object storage requires access to Minio, the Data warehouse team manages this set-up."}),"\n",(0,r.jsx)(a.li,{children:"Following the Minio URL provided allows access to create an AWS s3 \u2018bucket\u2019, which has some\r\nspecific naming conventions enforced and supplies some credentials to link to Dremio,"}),"\n",(0,r.jsx)(a.li,{children:"The Data Warehouse team can link this to object storage in Dremio, within Minio you can link or\r\nupload the object storage directly to the bucket created and once connected in Dremio, the data\r\nwill be available in the Dremio UI for querying."}),"\n"]}),"\n",(0,r.jsx)(a.p,{children:"Additionally, if required in edge-cases, users may need access to the virtual machine."}),"\n",(0,r.jsx)(a.h2,{id:"lakehouse-architecture",children:"Lakehouse Architecture"}),"\n",(0,r.jsx)(a.p,{children:"Data Lakehouse Architecture is a form of data management that intends to combine the best\r\nfeatures of traditional data lakes and data warehouses, this is ideal for Redback operations\r\nbecause as a company we share the same storage system (data lake) while needing\r\nindividualised places to store the data for each project."}),"\n",(0,r.jsx)(a.p,{children:"The benefits of Lakehouse architecture have been written about extensively and are utilised by\r\nmany elite companies however, some key reasons why Redback Operations specifically will\r\nbenefit from Lakehouse architecture over traditional data warehouse architecture are:"}),"\n",(0,r.jsxs)(a.p,{children:["\u27a2 Storage: the ability to store data in native formats, while remaining scalable\r\n\u27a2 Combined sources: one data Lakehouse can accommodate batch and streaming data sources.",(0,r.jsx)(a.br,{}),"\n","\u27a2 Better governance and schema enforcement: While maintaining flexibility around sources, data\r\nLakehouse\u2019s by design can better enforce formal mechanisms for storing and accessing data,\r\nwhich helps to keep the Lakehouse tidy and efficient, now and moving forward with member\r\nturnover.\r\n\u27a2 ML and data science integration: redback has a large contingent of analytics and data science\r\nusers, a data Lakehouse can be designed in a way to accommodate \u2018stages\u2019 of data from\r\ningestion for data engineers (data warehouse) and self-service analytics (Data science teams)\r\nwhile maintaining order and data integrity.\r\n\u27a2 Cost effective: Data Lakehouse architecture is free and can be built on open-source tools."]}),"\n",(0,r.jsx)(a.p,{children:"Finally, the prosed data pipeline ends with access to the data at any level of the medallion architecture.\r\nBronze being raw data, silver being transformed and cleaned data with any necessary merges or appends\r\nof other tables and gold a layer of that has analysis ready data with aggregations and a specific scope for\r\na more specific use. This concept is expanded on further later in the document."}),"\n",(0,r.jsx)(a.p,{children:"Initially we propose that one member of each team interface with Lakehouse to maintain governance over\r\nany changes. Having one representative per team ensures no duplication of efforts and a point of contact\r\nwhen tracing actions in the platform."}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.img,{alt:"image",src:t(18215).A+"",width:"851",height:"628"}),"\r\nEach team accesses the Data Lakehouse through a representative"]}),"\n",(0,r.jsx)(a.h2,{id:"modelling-architecture",children:"Modelling Architecture"}),"\n",(0,r.jsx)(a.p,{children:"Our architecture is made up of an arrangement of folders existing in an iceberg format, stored on a virtual\r\nmachine accessed and governed by ingestion, cataloging and Lakehouse platform tools."}),"\n",(0,r.jsx)(a.p,{children:"The layout is integral to the success of the data Lakehouse and is designed with each project redback\r\nspecific requirements in mind."}),"\n",(0,r.jsx)(a.p,{children:"The style of the architecture is known as medallion, Bronze, Silver, Gold."}),"\n",(0,r.jsx)(a.p,{children:"Medallion style architecture is a key aspect of data Lakehouse storage. Each stage (medallion) explained\r\nbriefly below:"}),"\n",(0,r.jsx)(a.p,{children:"Bronze: The bronze layer is effectively the Data Lake part of the Data Lakehouse. This is where source\r\ndata is stored as is, so there is consistent source of truth of where any data transformations can be traced\r\nback to. The bronze layer isn't accessible by end users and serves as a starting point for the silver and\r\ngold layers to build off. Bronze layer will become particularly valuable as a landing folder when\r\norchestration platforms are established and need somewhere to dump data to."}),"\n",(0,r.jsx)(a.p,{children:"Silver:    Flowing on from bronze layer, the silver layer is where the data is first transformed into Iceberg\r\ntables, this particular format is explained later in the document, but allows for historical version of data to\r\nbe stored as snapshots in the same file behind the current data, meaning that in silver we are able to\r\neffectively capture and itemise historical data for each source. Silver is also the layer involved in merging,\r\ntransforming and cleaning the data as tables to prepare them for the gold layer next."}),"\n",(0,r.jsx)(a.p,{children:"Gold:       The Gold layer of the data architecture is where the data that has been cleaned, prepared\r\nvalidated is stored, this data can be accessed directly queried or pushed downstream for use in models.\r\nThe point of contact for data users."}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.img,{alt:"image",src:t(67532).A+"",width:"1289",height:"647"}),"\r\nThe Data Lakehouse Layers as medallion architecture"]}),"\n",(0,r.jsx)(a.h2,{id:"folder-architecture",children:"Folder Architecture"}),"\n",(0,r.jsx)(a.p,{children:"The folder layout for the Data Lakehouse is important to allow for proper governance, by keeping a rigid\r\nfolder structure users are better able to build pipelines to orchestrate data and to find necessary data\r\nwhen required."}),"\n",(0,r.jsx)(a.p,{children:"In the event of an orchestration tool this folder style will be flexible to accommodate migration to a more\r\ntime focused folder layout as it already encourages naming data in a YYYY-MM-DD fashion."}),"\n",(0,r.jsx)(a.p,{children:"Below is the Data Lakehouse file structure."}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.img,{alt:"image",src:t(79157).A+"",width:"723",height:"751"}),"\r\nThe folder layout  for the Data Lakehouse"]}),"\n",(0,r.jsx)(a.h2,{id:"the-tech-stack",children:"The Tech Stack"}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.img,{alt:"image",src:t(10746).A+"",width:"1334",height:"610"}),"\r\nData flow through the Data Lakehouse tech stack"]}),"\n",(0,r.jsx)(a.h3,{id:"data-storage",children:"Data Storage"}),"\n",(0,r.jsx)(a.p,{children:"Our data storage is the Deakin university virtual machine server [ redback.it.deakin.edu.au ] with\r\n500gb of available storage, the VM server is Linux based and can be accessed through\r\ncommand prompt."}),"\n",(0,r.jsx)(a.p,{children:"It\u2019s important to regularly check the functionality of the VM. Its administration is outside of our\r\ncontrol and lies with Deakin IT department."}),"\n",(0,r.jsx)(a.p,{children:"Virtual machine is the BareMetal storage, in our on-prem architecture, typically and perhaps\r\nmoving forward could data storage would replace this as part of our data Lakehouse\r\narchitecture."}),"\n",(0,r.jsx)(a.h3,{id:"docker",children:"Docker"}),"\n",(0,r.jsx)(a.p,{children:"Docker containerises the Lakehouse tools and allows them to be utilised on non-Linux\r\nenvironments. Docker is crucial to running the Lakehouse platform and all the components are\r\nrun as part of a docker container."}),"\n",(0,r.jsx)(a.h3,{id:"minio",children:"MinIO"}),"\n",(0,r.jsx)(a.p,{children:"Minio is a object storage solution that acts like amazon s3 cloud storage and supplies a\r\ncompatible API for Dremio to connect to and store all forms of data and bring this through into\r\nthe Data Lakehouse. Minio compliments Dremio by having a focus on speed, a key requirement\r\nfor our use case and a reason Dremio was also chosen for the Lakehouse architecture."}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.img,{alt:"image",src:t(16931).A+"",width:"1139",height:"498"}),"\r\nMinio User Interface"]}),"\n",(0,r.jsx)(a.h3,{id:"nessie",children:"Nessie"}),"\n",(0,r.jsx)(a.p,{children:"Nessie is a Data Catalog, that records changes in the data or \u2018Transactions\u2019 as the\r\ndocumentation calls it. This program essentially gives the Lakehouse GitHub functionality by\r\nway of branching and commit actions. If there is a disaster, the Lakehouse and data can be\r\nrolled back to a state before any damage was made."}),"\n",(0,r.jsx)(a.h3,{id:"format",children:"Format"}),"\n",(0,r.jsx)(a.p,{children:"Apache Iceberg is a table format, specifically for Data Lakehouses, the format has a metadata\r\nlayer that the other tools utilise to enable versioning and make the tables that are Iceberg\r\nformat ACID compliant, that is basically ensuring that the file you access is correct and current."}),"\n",(0,r.jsx)(a.h3,{id:"dremio",children:"Dremio"}),"\n",(0,r.jsx)(a.p,{children:"Dremio is the UI and the Query engine, The UI allows access to the folders that are arranged in a\r\nstyle that is defined as a Data Lakehouse, this design aims to bring together the benefits of a\r\ndata lake and a data warehouse, basically the folders are layered and named with strict rules in\r\nan effort to make finding and querying the data needed as efficiently as possible."}),"\n",(0,r.jsx)(a.p,{children:"Dremio runs on Linux only and requires Docker to run the container. This setup is relatively short\r\nand doesn\u2019t require much upskilling. This can be the responsibility of a single admin to initially\r\nboot up and maintain governance of the container running on the VM, these are both important\r\nwhen handling-over responsibilities to incoming team members each Trimester moving\r\nforward."}),"\n",(0,r.jsx)(a.p,{children:"Dremio, when incorporated with Minio allows for unstructured or semi-structured data to be\r\nstored in the form of AWS S3 buckets, with all features typical of S3 storage."}),"\n",(0,r.jsx)(a.p,{children:"Nessie the catalog management tool, is leveraged with Dremio for its additional features of\r\ninbuilt catalog for versioning and rollback as well as metadata management important for data\r\ngovernance, ensuring each project, team and user can locate the necessary data they need."}),"\n",(0,r.jsx)(a.p,{children:"Dremio itself provides the user interface and query engine which is SQL compatible, the query\r\nlanguage for ad-hoc and source data retrieval."}),"\n",(0,r.jsxs)(a.p,{children:[(0,r.jsx)(a.img,{alt:"image",src:t(38600).A+"",width:"1258",height:"669"}),"\r\nDremio The Data Lakehouse platform"]}),"\n",(0,r.jsx)(a.h3,{id:"migration",children:"Migration"}),"\n",(0,r.jsx)(a.p,{children:"To migrate data from an existing source to the data Lakehouse involves a small number of steps.\r\nWhich as outlined at the beginning of this project, were to be kept as concise as possible to\r\nallow for ease of new users with each trimester\u2019s juniors."}),"\n",(0,r.jsx)(a.p,{children:"Currently to migrate data from an outside project to the Data Lakehouse requires creating the\r\nobject storage in Minio, gathering the access key and token, moving to Dremio and inserting\r\nthese credentials, where an object storage bucket is created and can be used for storage."}),"\n",(0,r.jsx)(a.p,{children:"This concept is covered in detail as part of the handover document."}),"\n",(0,r.jsx)(a.h2,{id:"orchestration-and-pipelines",children:"Orchestration and Pipelines"}),"\n",(0,r.jsx)(a.p,{children:"Currently, importing project data is a manual process, moving forward there is plenty of scope\r\nto accommodate pipelining and automation of ingestion. Setting up the data Lakehouse first\r\nbefore orchestrating source data is best-practice and Dremio platform has the capability to\r\naccept most sources and being open-source should accommodate a workaround if needed."}),"\n",(0,r.jsx)(a.h2,{id:"mongo-db",children:"Mongo DB"}),"\n",(0,r.jsx)(a.p,{children:"To help accommodate existing data processes, part of the overall Data Lakehouse platform will\r\ninclude MongoDB object storage. Using Mongo DB alongside the Data Lakehouse ensures there\r\nis no disruption to teams currently relying on MDB moving forward. Mongo DB is currently\r\nutilised as unstructured storage by some project teams."}),"\n",(0,r.jsx)(a.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(a.p,{children:"Our architecture plan extends beyond its current state. While we have the Dremio UI and the\r\nobject storage correctly functioning as a proof of concept with some project data existing in the\r\ndata Lakehouse further steps are required for full integration."}),"\n",(0,r.jsx)(a.p,{children:"The accessibility currently relies on the cooperation of Data warehouse staff. It would be best\r\npractice to arrange some governance around this."}),"\n",(0,r.jsx)(a.p,{children:"Most importantly, we need to incorporate an orchestration tool this will eliminate the manual\r\nprocesses involved in entering the data and remove potential for human error as part of this\r\nprocess. At the time of writing (TRI 1 2024) no decision has been made in regard to orchestration\r\nand has been raised as a possible project for Tri 2 2024."})]})}function l(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},18215:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/arch1-304fe0d1b840ddb55d6139f372b4c181.png"},67532:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/arch2-a2b6f570704271d0fd3c04964afd933b.png"},79157:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/arch3-9485d86b7e089fcac39136dfdf3eefdb.png"},10746:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/arch4-e0b8e0c3dc9601a90d88ff259058180d.png"},16931:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/arch5-3a55b41d07c8d4f6e2fdd719284f5f9c.png"},38600:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/arch6-6fc3831b5f7d0560b249357511a307e0.png"},28453:(e,a,t)=>{t.d(a,{R:()=>i,x:()=>s});var n=t(96540);const r={},o=n.createContext(r);function i(e){const a=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function s(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),n.createElement(o.Provider,{value:a},e.children)}}}]);