"use strict";(self.webpackChunkredback_documentation=self.webpackChunkredback_documentation||[]).push([[3486],{28144:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"data-warehousing/Instructional Documents/Redback Data Warehouse - Complete Guide","title":"Redback Data Warehouse - Complete Guide","description":"Last updated by \'18/05/2025\'","source":"@site/docs/data-warehousing/Instructional Documents/Redback Data Warehouse - Complete Guide.md","sourceDirName":"data-warehousing/Instructional Documents","slug":"/data-warehousing/Instructional Documents/Redback Data Warehouse - Complete Guide","permalink":"/redback-documentation/docs/data-warehousing/Instructional Documents/Redback Data Warehouse - Complete Guide","draft":false,"unlisted":false,"editUrl":"https://github.com/Redback-Operations/redback-documentation/blob/main/docs/data-warehousing/Instructional Documents/Redback Data Warehouse - Complete Guide.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Redback Data Warehouse - Complete Guide"},"sidebar":"tutorialSidebar","previous":{"title":"Onboarding","permalink":"/redback-documentation/docs/category/onboarding-1"},"next":{"title":"Data Warehouse Overview","permalink":"/redback-documentation/docs/data-warehousing/Instructional Documents/Data Warehouse Overview"}}');var r=s(74848),t=s(28453);const o={sidebar_position:1,title:"Redback Data Warehouse - Complete Guide"},a="Welcome to the Redback Data Warehouse Complete Technical Guide",l={},d=[{value:"The Virtual Machine (VM) \u2013 Core Infrastructure",id:"the-virtual-machine-vm--core-infrastructure",level:2},{value:"Key Characteristics",id:"key-characteristics",level:3},{value:"How the VM Powers the Warehouse",id:"how-the-vm-powers-the-warehouse",level:3},{value:"VM Access Overview",id:"vm-access-overview",level:3},{value:"Docker Notes",id:"docker-notes",level:3},{value:"Key Addresses",id:"key-addresses",level:3},{value:"Background \u2013 Requirements Gathering Summary",id:"background--requirements-gathering-summary",level:2},{value:"Key Pain Points Identified:",id:"key-pain-points-identified",level:3},{value:"Key Requirements Defined:",id:"key-requirements-defined",level:3},{value:"Data Architecture &amp; Platform Rationale",id:"data-architecture--platform-rationale",level:2},{value:"Why a Data Lakehouse?",id:"why-a-data-lakehouse",level:3},{value:"Tools Currently in Use (T1 2025)",id:"tools-currently-in-use-t1-2025",level:3},{value:"Medallion Architecture",id:"medallion-architecture",level:3},{value:"Typical Data Flow",id:"typical-data-flow",level:3},{value:"MinIO \u2013 Object Storage Backbone of the Data Warehouse",id:"minio--object-storage-backbone-of-the-data-warehouse",level:2},{value:"Accessing MinIO",id:"accessing-minio",level:3},{value:"Bucket Structure",id:"bucket-structure",level:3},{value:"How to Use MinIO \u2013 GUI vs Code",id:"how-to-use-minio--gui-vs-code",level:3},{value:"GUI Access (User-Friendly Option)",id:"gui-access-user-friendly-option",level:4},{value:"Programmatic Access (Recommended for Scripts)",id:"programmatic-access-recommended-for-scripts",level:4},{value:"Dremio \u2013 Interactive SQL Engine &amp; Lakehouse UI",id:"dremio--interactive-sql-engine--lakehouse-ui",level:2},{value:"Why Dremio?",id:"why-dremio",level:3},{value:"Accessing Dremio",id:"accessing-dremio",level:3},{value:"Connecting MinIO to Dremio (Source Setup)",id:"connecting-minio-to-dremio-source-setup",level:3},{value:"Creating Tables in Dremio (Two Ways)",id:"creating-tables-in-dremio-two-ways",level:3},{value:"Option 1: Using SQL Editor in GUI",id:"option-1-using-sql-editor-in-gui",level:4},{value:"Option 2: Scripted Pipeline",id:"option-2-scripted-pipeline",level:4},{value:"Using Flask API to Query Dremio via Code",id:"using-flask-api-to-query-dremio-via-code",level:3},{value:"Streamlit File Upload Service \u2013 Uploading &amp; Managing Files in the DW",id:"streamlit-file-upload-service--uploading--managing-files-in-the-dw",level:2},{value:"Accessing the File Upload Interface",id:"accessing-the-file-upload-interface",level:3},{value:"How to Upload a File",id:"how-to-upload-a-file",level:3},{value:"What about ETL?",id:"what-about-etl",level:3},{value:"Behind the Scenes: What Happens After Upload?",id:"behind-the-scenes-what-happens-after-upload",level:3},{value:"Downloading Files",id:"downloading-files",level:3},{value:"Option A: Programmatic Access via Flask API",id:"option-a-programmatic-access-via-flask-api",level:4},{value:"Option B: Using <code>curl</code> in Terminal",id:"option-b-using-curl-in-terminal",level:4},{value:"Option C: Download via the Streamlit UI",id:"option-c-download-via-the-streamlit-ui",level:4},{value:"Option D: Download directly from backend (Minio)",id:"option-d-download-directly-from-backend-minio",level:4},{value:"Other Available Services (Not in Full Production)",id:"other-available-services-not-in-full-production",level:2},{value:"MongoDB Connection Service",id:"mongodb-connection-service",level:2},{value:"Setup Overview (Admin/Dev Use)",id:"setup-overview-admindev-use",level:3},{value:"Mosquitto MQTT Broker (IoT Messaging)",id:"mosquitto-mqtt-broker-iot-messaging",level:2},{value:"What is MQTT?",id:"what-is-mqtt",level:3},{value:"Key Details",id:"key-details",level:3},{value:"Setup Notes",id:"setup-notes",level:3},{value:"Restic-Docker Backup System",id:"restic-docker-backup-system",level:2},{value:"Purpose",id:"purpose",level:3},{value:"What Gets Backed Up?",id:"what-gets-backed-up",level:3},{value:"Setup Overview (Admin Use Only)",id:"setup-overview-admin-use-only",level:3},{value:"Kafka + Airflow Stack",id:"kafka--airflow-stack",level:2},{value:"Core Use Case Pattern",id:"core-use-case-pattern",level:3},{value:"Common Use Cases for Other Teams",id:"common-use-cases-for-other-teams",level:3},{value:"Data Cleaning Pipelines",id:"data-cleaning-pipelines",level:4},{value:"ML Inference on Upload",id:"ml-inference-on-upload",level:4},{value:"Data Cataloging or Metadata Extraction",id:"data-cataloging-or-metadata-extraction",level:4},{value:"Real-Time Dashboard Updating",id:"real-time-dashboard-updating",level:4},{value:"How to Reuse This Pipeline",id:"how-to-reuse-this-pipeline",level:3},{value:"Step 1: Create a Kafka Topic (Optional)",id:"step-1-create-a-kafka-topic-optional",level:4},{value:"Step 2: Write Your Airflow DAG",id:"step-2-write-your-airflow-dag",level:4},{value:"Administrative Services Overview",id:"administrative-services-overview",level:2},{value:"Data Provenance Pipeline",id:"data-provenance-pipeline",level:3},{value:"Final Notes",id:"final-notes",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Last updated by:"})," daezel, ",(0,r.jsx)(n.strong,{children:"Last updated on:"})," '18/05/2025'"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Last updated by:"})," daezel, ",(0,r.jsx)(n.strong,{children:"Last updated on:"})," '18/05/2025'"]}),"\n",(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"welcome-to-the-redback-data-warehouse-complete-technical-guide",children:"Welcome to the Redback Data Warehouse Complete Technical Guide"})}),"\n",(0,r.jsxs)(n.p,{children:["This document serves as a ",(0,r.jsx)(n.strong,{children:"comprehensive and current overview"})," of the Redback Operations Data Warehouse ecosystem. Whether you're a new team member, a project contributor, or just exploring how our infrastructure works \u2014 this guide will help you understand what services we run, why we run them, and how to use them."]}),"\n",(0,r.jsxs)(n.p,{children:["This guide gives you the ",(0,r.jsx)(n.strong,{children:"big picture"})," \u2014 tying together MinIO, Dremio, the Streamlit File Upload System, Docker-based orchestration, and newer additions like Kafka, Airflow, and Restic \u2014 and will also contain reference links to all the individual service documentations in case there is a specific topic you need to explore in more detail."]}),"\n",(0,r.jsxs)(n.p,{children:["If you ever feel stuck, reach out to the ",(0,r.jsx)(n.strong,{children:"Data Warehouse leadership"})," or your ",(0,r.jsx)(n.strong,{children:"mentor"}),". And if you spot anything missing or unclear, we encourage you to contribute!"]}),"\n",(0,r.jsx)(n.p,{children:"Happy exploring !!"}),"\n",(0,r.jsx)(n.h2,{id:"the-virtual-machine-vm--core-infrastructure",children:"The Virtual Machine (VM) \u2013 Core Infrastructure"}),"\n",(0,r.jsxs)(n.p,{children:["The Redback Data Warehouse is hosted entirely on a dedicated ",(0,r.jsx)(n.strong,{children:"Deakin on-premises virtual machine"})," (VM), located within the university's infrastructure. This VM serves as the ",(0,r.jsx)(n.strong,{children:"central hub"})," for all data warehouse services and storage."]}),"\n",(0,r.jsx)(n.h3,{id:"key-characteristics",children:"Key Characteristics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Linux-based OS"})," (Ubuntu) maintained by Deakin IT"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"500GB local storage capacity"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No cloud capabilities"})," \u2013 all tools must be deployable on bare-metal"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Docker-powered environment"})," \u2013 all services run in containers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Shared system"})," \u2013 used by multiple projects and users (cross-team)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"how-the-vm-powers-the-warehouse",children:"How the VM Powers the Warehouse"}),"\n",(0,r.jsx)(n.p,{children:"The VM hosts and runs all critical tools and services, including:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"MinIO (object storage)"}),"\n",(0,r.jsx)(n.li,{children:"Dremio (query engine)"}),"\n",(0,r.jsx)(n.li,{children:"MongoDB (NoSQL database)"}),"\n",(0,r.jsx)(n.li,{children:"Streamlit File Upload Service"}),"\n",(0,r.jsx)(n.li,{children:"Flask APIs"}),"\n",(0,r.jsx)(n.li,{children:"Apache Airflow & Kafka (ETL & streaming)"}),"\n",(0,r.jsx)(n.li,{children:"Spark, Elasticsearch, Postgres, and more"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Each of these services runs in an isolated ",(0,r.jsx)(n.strong,{children:"Docker container"}),". This allows for flexible deployment, upgrades, and modular management \u2014 but comes with the caveat that ",(0,r.jsx)(n.strong,{children:"any restart or misconfiguration affects the shared production environment."})]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"VM operations require careful coordination. If you're unsure, check with a DW leader before restarting, modifying, or adding containers."}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["\u26a0\ufe0f",(0,r.jsx)(n.strong,{children:"Important Warning \u2013 Shared VM Environment"})]}),"\n",(0,r.jsxs)(n.p,{children:["The Redback Data Warehouse runs on a ",(0,r.jsx)(n.strong,{children:"shared production Virtual Machine (VM)"})," used by all teams. Any changes to services or containers affect everyone using the system."]}),"\n",(0,r.jsxs)(n.p,{children:["\u26a0\ufe0f ",(0,r.jsx)(n.strong,{children:"Do NOT run destructive or high-impact commands"})," like ",(0,r.jsx)(n.code,{children:"docker rm"}),", ",(0,r.jsx)(n.code,{children:"docker volume rm"}),", or ",(0,r.jsx)(n.code,{children:"docker-compose down"})," without first consulting the ",(0,r.jsx)(n.strong,{children:"Data Warehouse team lead"})," or your ",(0,r.jsx)(n.strong,{children:"mentor"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Such actions can:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Permanently delete project data or volumes"}),"\n",(0,r.jsx)(n.li,{children:"Break core infrastructure (MinIO, Dremio, Kafka, etc.)"}),"\n",(0,r.jsx)(n.li,{children:"Disrupt other projects that rely on running containers"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Always coordinate changes or troubleshooting with team leadership. If unsure, ask first."}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"vm-access-overview",children:"VM Access Overview"}),"\n",(0,r.jsx)(n.p,{children:"Access is available to authorized users only via VPN + SSH:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Connect to Deakin VPN using Cisco AnyConnect"}),"\n",(0,r.jsxs)(n.li,{children:["Access via terminal (",(0,r.jsx)(n.code,{children:"ssh yourusername@redback.it.deakin.edu.au"}),") or VSCode Remote SSH"]}),"\n",(0,r.jsx)(n.li,{children:"Recommended: Create a personal working folder inside the VM and clone the GitHub repo"}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["Admins can create new VM users via ",(0,r.jsx)(n.code,{children:"sudo adduser <username>"}),", but credentials must be shared securely."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"docker-notes",children:"Docker Notes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["All services are listed via ",(0,r.jsx)(n.code,{children:"docker ps"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Restart core infrastructure with ",(0,r.jsx)(n.code,{children:"docker compose up -d"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Persistent Volumes"})," are configured for critical tools like Dremio, MinIO, and Postgres to retain data even on restart"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"key-addresses",children:"Key Addresses"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Service"}),(0,r.jsx)(n.th,{children:"URL/Port"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"VM SSH"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"ssh <user>@redback.it.deakin.edu.au"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"File Upload Service"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"http://10.137.0.149:80/",children:"http://10.137.0.149:80/"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MinIO"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"http://10.137.0.149:9001/login",children:"http://10.137.0.149:9001/login"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Dremio"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"http://10.137.0.149:9047/",children:"http://10.137.0.149:9047/"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MongoDB API"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"http://10.137.0.149:5003/documents",children:"http://10.137.0.149:5003/documents"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Spark Jupyter Notebooks"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"http://10.137.0.149:8888/",children:"http://10.137.0.149:8888/"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Flask API (Downloads)"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"http://10.137.0.149:5000/",children:"http://10.137.0.149:5000/"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Kibana (Logs - if active)"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"http://10.137.0.149:5601/",children:"http://10.137.0.149:5601/"})})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["See full access steps and Docker commands in the ",(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Instructional%20Documents/VM%20Guide",children:"VM Access Guide (ONB2)"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"background--requirements-gathering-summary",children:"Background \u2013 Requirements Gathering Summary"}),"\n",(0,r.jsxs)(n.p,{children:["Before building the Redback Data Warehouse infrastructure, a structured ",(0,r.jsx)(n.strong,{children:"requirements gathering process"})," was conducted through stakeholder meetings and surveys. The goal was to understand project-level and company-wide needs and evaluate suitable Data Lakehouse solutions."]}),"\n",(0,r.jsx)(n.h3,{id:"key-pain-points-identified",children:"Key Pain Points Identified:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"No centralized platform for storing and accessing company-wide data."}),"\n",(0,r.jsx)(n.li,{children:"Frequent issues with updating datasets (manual Git workflows)."}),"\n",(0,r.jsxs)(n.li,{children:["Need for ",(0,r.jsx)(n.strong,{children:"supporting both structured and unstructured data"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Budget constraints and student turnover requiring low-code, easy-to-learn tools."}),"\n",(0,r.jsx)(n.li,{children:"Licensing issues tied to individual users \u2014 not scalable for rotating student teams."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"key-requirements-defined",children:"Key Requirements Defined:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Must scale to accommodate multiple projects."}),"\n",(0,r.jsx)(n.li,{children:"Supports CSV, JSON, media, and other object formats."}),"\n",(0,r.jsx)(n.li,{children:"Low technical entry barrier (GUI, minimal setup)."}),"\n",(0,r.jsx)(n.li,{children:"Free/open-source or trial-based licensing preferred."}),"\n",(0,r.jsx)(n.li,{children:"Centralized data access and governance."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Based on these, tools like ",(0,r.jsx)(n.strong,{children:"MinIO, Dremio, MongoDB, and Apache Airflow"})," were selected and tested."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Data%20Lakehouse/Data%20Warehouse%20Requirements",children:"View Full Requirements Document"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"data-architecture--platform-rationale",children:"Data Architecture & Platform Rationale"}),"\n",(0,r.jsxs)(n.p,{children:["Redback Operations adopted a ",(0,r.jsx)(n.strong,{children:"Data Lakehouse architecture"})," in Trimester 1, 2024, after operating without a centralized data platform. Initial requirements were gathered through interviews and surveys with project and company leaders, which led to a focus on open-source, scalable, and VM-compatible technologies."]}),"\n",(0,r.jsx)(n.p,{children:"Since then, the platform has grown significantly and now supports real-time pipelines, orchestration tools, automated backups, and enhanced governance layers \u2014 all running on a shared Dockerized VM environment."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"why-a-data-lakehouse",children:"Why a Data Lakehouse?"}),"\n",(0,r.jsx)(n.p,{children:"A Data Lakehouse combines the flexibility of a Data Lake with the governance features of a Data Warehouse. This hybrid design supports:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Native file/object storage (CSV, JSON, images, etc.)"}),"\n",(0,r.jsx)(n.li,{children:"SQL-compatible querying (via Dremio)"}),"\n",(0,r.jsx)(n.li,{children:"Versioning and rollback (via Apache Iceberg + Nessie)"}),"\n",(0,r.jsx)(n.li,{children:"Cost-effective, on-premises deployment on the Deakin VM"}),"\n",(0,r.jsx)(n.li,{children:"Centralization for collaboration across rotating student teams"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"tools-currently-in-use-t1-2025",children:"Tools Currently in Use (T1 2025)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Tool / Service"}),(0,r.jsx)(n.th,{children:"Role in Ecosystem"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Dremio"})}),(0,r.jsx)(n.td,{children:"SQL query engine + GUI frontend for browsing and querying data"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"MinIO"})}),(0,r.jsx)(n.td,{children:"S3-compatible object storage backend for both raw (Bronze) and cleaned (Silver) data"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Streamlit App"})}),(0,r.jsx)(n.td,{children:"Upload interface for data files with optional preprocessing"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"MongoDB"})}),(0,r.jsx)(n.td,{children:"JSON storage for semi-structured data used by various projects"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"PostgreSQL"})}),(0,r.jsx)(n.td,{children:"Provenance metadata store (used in ELK pipelines)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Apache Iceberg"})}),(0,r.jsx)(n.td,{children:"ACID-compliant table format for versioned datasets in Dremio"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Nessie"})}),(0,r.jsx)(n.td,{children:"Metadata catalog and rollback manager for Dremio (experimental stage)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Flask API"})}),(0,r.jsx)(n.td,{children:"Enables safe, secure data querying and file downloads via HTTP"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Kafka"})}),(0,r.jsx)(n.td,{children:"Message broker used for real-time pipelines (e.g., Project 4 image ingestion)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Apache Airflow"})}),(0,r.jsx)(n.td,{children:"Orchestrates processing pipelines triggered by Kafka or scheduled jobs"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"FastAPI"})}),(0,r.jsx)(n.td,{children:"API layer connecting frontend uploads to backend workflows (Kafka + Airflow)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Docker"})}),(0,r.jsx)(n.td,{children:"Container orchestration for all services running on the shared production VM"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Restic"})}),(0,r.jsx)(n.td,{children:"Docker-based backup solution for key volumes and containers"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Spark Notebooks"})}),(0,r.jsx)(n.td,{children:"Jupyter + Spark environment (available, but not in active use)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"ClamAV"})}),(0,r.jsx)(n.td,{children:"Virus scanner container for uploaded files (currently experimental)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Wazuh"})}),(0,r.jsx)(n.td,{children:"Security monitoring suite (shared with Cybersecurity team on VM)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Elasticsearch"})}),(0,r.jsx)(n.td,{children:"Indexes logs from all tools (provenance)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Logstash"})}),(0,r.jsx)(n.td,{children:"Pipeline to parse & forward logs"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Kibana"})}),(0,r.jsx)(n.td,{children:"Log dashboard/visualisation tool (not active)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"PostgreSQL (Provenance)"})}),(0,r.jsx)(n.td,{children:"Structured storage of event logs"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Restic"})}),(0,r.jsx)(n.td,{children:"Docker volume backup system"})]})]})]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["All services are deployed on a shared Linux-based VM (",(0,r.jsx)(n.code,{children:"redback.it.deakin.edu.au"}),") using Docker. Admin rights and service restarts should be handled with care."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"medallion-architecture",children:"Medallion Architecture"}),"\n",(0,r.jsx)(n.p,{children:"Redback follows the Medallion Lakehouse model \u2014 a layered storage strategy:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Bronze Layer"}),": Raw, unprocessed source data. Not user-editable."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Silver Layer"}),": Cleaned and transformed data. Stored in Iceberg format for versioning."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gold Layer"}),": Final, analysis-ready data with specific aggregations or scopes."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Folder layout follows:",(0,r.jsx)(n.br,{}),"\n",(0,r.jsx)(n.code,{children:"/project_name/YYYY/tX_task_name/student_ID (optional)"})]}),"\n",(0,r.jsx)(n.p,{children:"This standard helps with both governance and future orchestration."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"typical-data-flow",children:"Typical Data Flow"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Upload"})," data via Streamlit (or direct upload to MinIO)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Store"})," in Bronze bucket (raw layer)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ETL pipeline"})," processes file into Silver bucket (cleaned layer)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Register/query"})," data using Dremio (optionally build Gold tables/views)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"(Optional)"}),": Use Airflow/Kafka for automated workflows"]}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Data%20Lakehouse/Data%20Architecture",children:"View full Data Architecture"})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"minio--object-storage-backbone-of-the-data-warehouse",children:"MinIO \u2013 Object Storage Backbone of the Data Warehouse"}),"\n",(0,r.jsxs)(n.p,{children:["MinIO is the core object storage system used in the Redback Data Warehouse infrastructure. It functions similarly to AWS S3, allowing users to store and retrieve files through programmatic APIs or a friendly web-based GUI. It plays a ",(0,r.jsx)(n.strong,{children:"central role in storing raw and processed datasets"}),", especially CSV, TXT, JSON, XLSX files used in the File Upload Service and downstream tools like Dremio."]}),"\n",(0,r.jsx)(n.h3,{id:"accessing-minio",children:"Accessing MinIO"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"URL:"})," ",(0,r.jsx)(n.code,{children:"http://10.137.0.149:9000"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Admin Portal:"})," ",(0,r.jsx)(n.code,{children:"http://10.137.0.149:9001"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Credentials:"})," Not included here; contact the DW Team Leader or Mentor for access."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'Once logged in, users will see a UI displaying MinIO "buckets" \u2014 essentially folders for storing categorized files.'}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Default Bucket for Uploads:"})," ",(0,r.jsx)(n.code,{children:"dw-bronze-bucket"}),(0,r.jsx)(n.br,{}),"\n","Files uploaded via the Streamlit File Upload App are saved here before preprocessing or ETL."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"bucket-structure",children:"Bucket Structure"}),"\n",(0,r.jsxs)(n.p,{children:["MinIO separates data into ",(0,r.jsx)(n.strong,{children:"Bronze"})," and ",(0,r.jsx)(n.strong,{children:"Silver"})," tiers:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Bronze Bucket:"})," Stores raw, unprocessed uploads."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Silver Bucket:"})," Stores cleaned, preprocessed files after ETL transformations."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["This clean separation supports better ",(0,r.jsx)(n.strong,{children:"data governance"})," and makes it easy for tools like Dremio to distinguish between staging and production data."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Bucket Name"}),(0,r.jsx)(n.th,{children:"Purpose"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"dw-bronze-bucket"})}),(0,r.jsx)(n.td,{children:"Raw files uploaded via UI or manually"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"dw-silver-bucket"})}),(0,r.jsx)(n.td,{children:"Cleaned/preprocessed data from ETL"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"how-to-use-minio--gui-vs-code",children:"How to Use MinIO \u2013 GUI vs Code"}),"\n",(0,r.jsx)(n.h4,{id:"gui-access-user-friendly-option",children:"GUI Access (User-Friendly Option)"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Navigate to ",(0,r.jsx)(n.a,{href:"http://10.137.0.149:9001",children:"MinIO Admin Portal"})]}),"\n",(0,r.jsx)(n.li,{children:"Login using your assigned credentials."}),"\n",(0,r.jsx)(n.li,{children:"Browse, upload, delete, or organize files within project-specific folders."}),"\n",(0,r.jsx)(n.li,{children:"You can also generate new Access Keys and Secret Keys for programmatic use."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"programmatic-access-recommended-for-scripts",children:"Programmatic Access (Recommended for Scripts)"}),"\n",(0,r.jsxs)(n.p,{children:["MinIO can be accessed using the official Python SDK (",(0,r.jsx)(n.code,{children:"minio"}),"), behaving like AWS S3."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from minio import Minio\r\nimport os\r\n\r\nclient = Minio(\r\n    "10.137.0.149:9000",\r\n    access_key=os.getenv("AWS_ACCESS_KEY"),\r\n    secret_key=os.getenv("AWS_SECRET_KEY"),\r\n    secure=False\r\n)\r\n\n'})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Instructional%20Documents/MinIO%20Guide",children:"click here to view minio guide"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"dremio--interactive-sql-engine--lakehouse-ui",children:"Dremio \u2013 Interactive SQL Engine & Lakehouse UI"}),"\n",(0,r.jsxs)(n.p,{children:["Dremio is the ",(0,r.jsx)(n.strong,{children:"query engine and interactive layer"})," for the Redback Data Warehouse. It connects to data stored in MinIO and MongoDB, allowing users to create virtual SQL tables and run queries directly from a browser or code interface."]}),"\n",(0,r.jsxs)(n.p,{children:["Unlike traditional database engines, Dremio reads directly from ",(0,r.jsx)(n.strong,{children:"object storage"})," (MinIO), and allows teams to build ",(0,r.jsx)(n.strong,{children:"virtual tables"}),' ("views") on top of raw data \u2014 without duplication. Its graphical interface and SQL endpoint make it easy to use for both analysts and developers.']}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"why-dremio",children:"Why Dremio?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost-effective"}),": Free and open-source."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"User-friendly"}),": Comes with a full GUI and SQL editor."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-source capable"}),": Can query data from MinIO, MongoDB, PostgreSQL, and more."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time-travel support"}),": Enables data versioning with formats like Apache Iceberg and Parquet."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safe query execution"}),": Uses a proxy Flask API to limit risky SQL commands."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"accessing-dremio",children:"Accessing Dremio"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dremio Web UI"}),": ",(0,r.jsx)(n.code,{children:"http://10.137.0.149:9047"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Flask SQL API"}),": ",(0,r.jsx)(n.code,{children:"http://10.137.0.149:5001/dremio_query"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access"}),": VPN required (Cisco AnyConnect)",(0,r.jsx)(n.br,{}),"\n","Login credentials are provided by the Data Warehouse team leader or mentor."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"connecting-minio-to-dremio-source-setup",children:"Connecting MinIO to Dremio (Source Setup)"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["In the Dremio UI, click ",(0,r.jsx)(n.strong,{children:"\u201cAdd Source\u201d"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Select ",(0,r.jsx)(n.strong,{children:"Amazon S3"})," as the source type."]}),"\n",(0,r.jsxs)(n.li,{children:["Use the ",(0,r.jsx)(n.strong,{children:"Access Key"})," and ",(0,r.jsx)(n.strong,{children:"Secret Key"})," from MinIO to authenticate."]}),"\n",(0,r.jsxs)(n.li,{children:["In ",(0,r.jsx)(n.strong,{children:"Advanced Options"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Tick ",(0,r.jsx)(n.code,{children:"Enable Compatibility Mode"})]}),"\n",(0,r.jsxs)(n.li,{children:["Provide the bucket path (e.g., ",(0,r.jsx)(n.code,{children:"/project-2"}),")"]}),"\n",(0,r.jsx)(n.li,{children:"Add required connection properties."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Click ",(0,r.jsx)(n.strong,{children:"Save"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Once saved, the MinIO bucket will be visible as a data source within Dremio\u2019s ",(0,r.jsx)(n.strong,{children:"\u201cSources\u201d"})," tab."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"creating-tables-in-dremio-two-ways",children:"Creating Tables in Dremio (Two Ways)"}),"\n",(0,r.jsx)(n.h4,{id:"option-1-using-sql-editor-in-gui",children:"Option 1: Using SQL Editor in GUI"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Navigate to the SQL tab in Dremio."}),"\n",(0,r.jsxs)(n.li,{children:["Write a SQL ",(0,r.jsx)(n.code,{children:"SELECT"}),", ",(0,r.jsx)(n.code,{children:"JOIN"}),", or ",(0,r.jsx)(n.code,{children:"CREATE VIEW"})," statement."]}),"\n",(0,r.jsx)(n.li,{children:"Save the view as a virtual table to use in future queries."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"option-2-scripted-pipeline",children:"Option 2: Scripted Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:["A Python script (",(0,r.jsx)(n.code,{children:"pipeline.py"}),") exists in the Redback repository (currently in a forked branch) which:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Takes a CSV file from MinIO"}),"\n",(0,r.jsx)(n.li,{children:"Automatically creates a Dremio SQL view"}),"\n",(0,r.jsx)(n.li,{children:"Registers it for downstream queries"}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"This automates ingestion + query registration for power users and is expected to be merged soon into the official repo."}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"using-flask-api-to-query-dremio-via-code",children:"Using Flask API to Query Dremio via Code"}),"\n",(0,r.jsxs)(n.p,{children:["Redback also provides a ",(0,r.jsx)(n.strong,{children:"Flask API interface"})," that lets users query Dremio securely using only ",(0,r.jsx)(n.code,{children:"SELECT"})," statements."]}),"\n",(0,r.jsx)(n.p,{children:"Here\u2019s an example Jupyter notebook workflow:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import requests\r\nimport json\r\nimport pandas as pd\r\n\r\napi_url = "http://10.137.0.149:5001/dremio_query"\r\nheaders = { "Content-Type": "application/json" }\r\n\r\nsql_query = {\r\n    "sql": "SELECT * FROM \\"project-3\\".\\"extended_activities\\" LIMIT 10;"\r\n}\r\n\r\nresponse = requests.post(api_url, headers=headers, data=json.dumps(sql_query))\r\nresult = response.json()\r\n\r\ndf = pd.DataFrame(result[\'rows\'])\r\ndisplay(df)\n'})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Instructional%20Documents/Dremio%20Guide",children:"click here to view dremio guide"})}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Dremio/Managing-the-structured-solution",children:"Other useful documents - maintaining structured dremio solution"})}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Dremio/Dremio-API",children:"other useful documents - how to access stored data in dremio"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"streamlit-file-upload-service--uploading--managing-files-in-the-dw",children:"Streamlit File Upload Service \u2013 Uploading & Managing Files in the DW"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"File Upload Service (FUS)"})," is a web-based interface built using Streamlit. It allows users to upload CSV files to the Data Warehouse in a structured and governed way. Uploaded files are stored in ",(0,r.jsx)(n.strong,{children:"MinIO buckets"})," under project-specific folders and optionally processed before storage."]}),"\n",(0,r.jsx)(n.p,{children:"This service is especially useful for team members who need a simple way to upload, clean, and manage datasets without directly interacting with the VM or MinIO backend."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"accessing-the-file-upload-interface",children:"Accessing the File Upload Interface"}),"\n",(0,r.jsx)(n.p,{children:"The Streamlit app is hosted on the Redback Data Warehouse Virtual Machine."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"URL"}),": ",(0,r.jsx)(n.code,{children:"http://10.137.0.149/"})," (default root port (80))"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VPN Required"}),": Connect using Cisco AnyConnect to access the internal network."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Authentication"}),": Once VPN is active, no additional login is required for the app."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"how-to-upload-a-file",children:"How to Upload a File"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Select Your Project"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Choose the project folder where your file will be stored (e.g., ",(0,r.jsx)(n.code,{children:"project-1"}),", ",(0,r.jsx)(n.code,{children:"project-3"}),")."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Upload File"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Drag and drop a file or browse for one."}),"\n",(0,r.jsx)(n.li,{children:"At the time of writing, the file formats supported by file upload service are - CSV, TXT, JSON, XLSX"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Choose Preprocessing (Optional)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"None"}),": File is uploaded as-is."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Clean-Up"}),": Removes empty columns, duplicate rows, standardizes format."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Machine Learning Prep"}),": Scales numeric features, handles missing values, optimizes for ML tasks."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Filename Options"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["By default, a ",(0,r.jsx)(n.strong,{children:"prefix"})," and ",(0,r.jsx)(n.strong,{children:"timestamp suffix"})," are added (recommended for governance)."]}),"\n",(0,r.jsx)(n.li,{children:"Unticking the box allows you to overwrite an existing file by using the same filename."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Submit Upload"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Click the ",(0,r.jsx)(n.strong,{children:"Upload to Data Warehouse"})," button."]}),"\n",(0,r.jsx)(n.li,{children:"A success or error message will appear."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["Uploaded files are stored in the ",(0,r.jsx)(n.code,{children:"dw-bronze-bucket"})," in MinIO. If preprocessing is applied, the cleaned version is saved in the ",(0,r.jsx)(n.code,{children:"dw-silver-bucket"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"what-about-etl",children:"What about ETL?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ETL is a separate trigger, not automatically tied to upload."}),"\n",(0,r.jsx)(n.li,{children:"In most cases, ETL also reads from dw-bronze-bucket, transforms the data, and may overwrite or add new files to Silver \u2014 depending on the pipeline setup."}),"\n",(0,r.jsx)(n.li,{children:"If preprocessing wasn't selected during upload, then Silver won't have that file until ETL moves it there."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"behind-the-scenes-what-happens-after-upload",children:"Behind the Scenes: What Happens After Upload?"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Uploaded file \u2192 stored in ",(0,r.jsx)(n.strong,{children:"MinIO"})," under selected project folder."]}),"\n",(0,r.jsxs)(n.li,{children:["If preprocessing is selected:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A Python script processes the file using ",(0,r.jsx)(n.strong,{children:"Pandas"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Resulting cleaned file is uploaded to ",(0,r.jsx)(n.code,{children:"dw-silver-bucket"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Metadata is appended (e.g., extract date, unique ID)."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"downloading-files",children:"Downloading Files"}),"\n",(0,r.jsx)(n.h4,{id:"option-a-programmatic-access-via-flask-api",children:"Option A: Programmatic Access via Flask API"}),"\n",(0,r.jsx)(n.p,{children:"You can retrieve files from the VM using the Flask API:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example \u2013 Using Python:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import requests\r\n\r\nurl = "http://10.137.0.149:5000/download-file"\r\nparams = {\r\n    "bucket": "dw-bucket-bronze",\r\n    "filename": "project3/testdocument_20240921.csv"\r\n}\r\n\r\nresponse = requests.get(url, params=params)\r\nwith open("downloaded_file.csv", "wb") as f:\r\n    f.write(response.content)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["We are showing the ",(0,r.jsx)(n.strong,{children:"Bronze bucket"})," here, but it works exactly the same for Silver too \u2014 just change the bucket name."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.h4,{id:"option-b-using-curl-in-terminal",children:["Option B: Using ",(0,r.jsx)(n.code,{children:"curl"})," in Terminal"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -o file.csv "http://10.137.0.149:5000/download-file?bucket=dw-bucket-bronze&filename=project3/testdocument_20240921.csv"\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h4,{id:"option-c-download-via-the-streamlit-ui",children:"Option C: Download via the Streamlit UI"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Go to: ",(0,r.jsx)(n.code,{children:"http://10.137.0.149/"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Select the appropriate tab:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"View Original Files"})," (Bronze bucket)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"View Pre-Processed Files"})," (Silver bucket)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Choose the relevant project from the dropdown."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Select a file and click ",(0,r.jsx)(n.strong,{children:"Download"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"When you click the download button, the app shows the corresponding Flask API URL used under the hood."}),"\n",(0,r.jsx)(n.h4,{id:"option-d-download-directly-from-backend-minio",children:"Option D: Download directly from backend (Minio)"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Navigate to correct folder/file and manually download file."}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Instructional%20Documents/How%20To%20Access%20The%20File%20Upload%20Service",children:"click here to view file upload service guide"})}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["[streamlit setup documentation] (",(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Streamlit%20tutorial/streamlit_tutorial",children:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Streamlit%20tutorial/streamlit_tutorial"}),")  - In case you need to work on implementing some upgrades in the existing file upload service or due something of your own using streamlit, this guide will be handy"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"other-available-services-not-in-full-production",children:"Other Available Services (Not in Full Production)"}),"\n",(0,r.jsx)(n.p,{children:"The following services are deployed on the VM but are not currently in active production use. They may be utilized in future phases or specialized projects."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Project Nessie"}),(0,r.jsx)(n.br,{}),"\n","A metadata store integrated with Dremio. Enables version control and historical tracking of datasets. Proof of concept complete, but not actively used yet."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Apache Spark (Jupyter Notebooks)"}),(0,r.jsx)(n.br,{}),"\n","Jupyter environment running on the VM at ",(0,r.jsx)(n.code,{children:"http://10.137.0.149:8888/"}),". Supports distributed Spark jobs for large-scale data processing. Currently idle due to lack of large production datasets."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Instructional%20Documents/Not%20in%20Prod",children:"click here to view documentation on additional services"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"mongodb-connection-service",children:"MongoDB Connection Service"}),"\n",(0,r.jsxs)(n.p,{children:["MongoDB is used within the Data Warehouse for storing ",(0,r.jsx)(n.strong,{children:"semi-structured data"}),", such as ",(0,r.jsx)(n.code,{children:".json"})," documents. To make this easier for teams to use, a dedicated ",(0,r.jsx)(n.strong,{children:"web server/API"})," has been set up that interacts with MongoDB using RESTful endpoints."]}),"\n",(0,r.jsx)(n.h3,{id:"setup-overview-admindev-use",children:"Setup Overview (Admin/Dev Use)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Repository: ",(0,r.jsx)(n.code,{children:"redback-data-warehouse/MongoDB Connection"})]}),"\n",(0,r.jsxs)(n.li,{children:["Services are containerized using ",(0,r.jsx)(n.strong,{children:"Docker Compose"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:".env"})," file must include:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"MONGO_URI"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"DB_NAME"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"COLLECTION_NAME"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Run with:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker-compose up --build\r\n\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/MongoDb%20Connection/mongodbconnection",children:"click here to view detailed guide on MongoDB"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"mosquitto-mqtt-broker-iot-messaging",children:"Mosquitto MQTT Broker (IoT Messaging)"}),"\n",(0,r.jsxs)(n.p,{children:["The Data Warehouse VM hosts a lightweight ",(0,r.jsx)(n.strong,{children:"Mosquitto MQTT Broker"}),", primarily for ",(0,r.jsx)(n.strong,{children:"real-time messaging"})," and ",(0,r.jsx)(n.strong,{children:"IoT simulation use cases"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"what-is-mqtt",children:"What is MQTT?"}),"\n",(0,r.jsx)(n.p,{children:"MQTT (Message Queuing Telemetry Transport) is a lightweight protocol ideal for low-overhead, publish/subscribe-based communication. It\u2019s commonly used in IoT environments or systems requiring fast, low-bandwidth updates."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"key-details",children:"Key Details"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Broker IP"}),": ",(0,r.jsx)(n.code,{children:"10.137.0.149"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Port"}),": ",(0,r.jsx)(n.code,{children:"1883"})," (default, unencrypted)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Anonymous Access"}),": Enabled"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Persistence"}),": Enabled (messages survive broker restarts)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Logs"}),": ",(0,r.jsx)(n.code,{children:"/var/log/mosquitto/mosquitto.log"})]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"setup-notes",children:"Setup Notes"}),"\n",(0,r.jsx)(n.p,{children:"Installed via:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"sudo apt update\r\nsudo apt install -y mosquitto mosquitto-clients\r\nsudo systemctl enable mosquitto\r\nsudo systemctl start mosquitto\n"})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Mosquitto/mosquitto_documentation",children:"Click here to view guide on Mosquitto MQTT"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"restic-docker-backup-system",children:"Restic-Docker Backup System"}),"\n",(0,r.jsxs)(n.p,{children:["The Data Warehouse includes an automated backup system using ",(0,r.jsx)(n.strong,{children:"Restic"})," \u2014 a secure, open-source tool for backing up Docker volumes on the VM."]}),"\n",(0,r.jsx)(n.h3,{id:"purpose",children:"Purpose"}),"\n",(0,r.jsxs)(n.p,{children:["This system ensures that critical services like ",(0,r.jsx)(n.strong,{children:"MinIO"}),", ",(0,r.jsx)(n.strong,{children:"PostgreSQL"}),", ",(0,r.jsx)(n.strong,{children:"Dremio"}),", and ",(0,r.jsx)(n.strong,{children:"Elasticsearch"})," are backed up regularly. If a container crashes or data is accidentally lost, snapshots allow for fast recovery."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"what-gets-backed-up",children:"What Gets Backed Up?"}),"\n",(0,r.jsx)(n.p,{children:"Restic monitors and stores snapshots of the following Docker volumes:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"data-lakehouse_minio-data"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"data-lakehouse_minio-config"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"fileuploadservice_dremio-data"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"dp-postgres-data"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"dp-es-data"})," ",(0,r.jsx)(n.em,{children:"(Elasticsearch)"})]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"dp-logstash-data"})}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"All these volumes are tied to key components of the data platform (storage, search, provenance, etc.)."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"setup-overview-admin-use-only",children:"Setup Overview (Admin Use Only)"}),"\n",(0,r.jsx)(n.p,{children:"To deploy the Restic backup system:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Clone the repo"})," and navigate to the Restic directory:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/Redback-Operations/redback-data-warehouse.git\r\ncd restic\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create required volumes"})," ",(0,r.jsx)(n.em,{children:"(if not already present)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"data-lakehouse_minio-data"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"fileuploadservice_dremio-data"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"dp-postgres-data"}),", etc."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Add your Restic password"})," to ",(0,r.jsx)(n.code,{children:"restic-password.txt"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Start the service"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"chmod +x scripts/backup.sh\r\ndocker-compose up -d\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Check logs or restore snapshots"})," using:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker exec -it restic-backup sh\r\nrestic snapshots\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["For complete instructions, including customisation and snapshot restoration, refer to the ",(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Restic/",children:"Restic Full Guide"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"kafka--airflow-stack",children:"Kafka + Airflow Stack"}),"\n",(0,r.jsxs)(n.p,{children:["A Kafka-based real-time pipeline is now operational on the VM, originally developed for Project 4. This stack integrates ",(0,r.jsx)(n.strong,{children:"FastAPI"}),", ",(0,r.jsx)(n.strong,{children:"Kafka"}),", and ",(0,r.jsx)(n.strong,{children:"Airflow"}),", and it's designed to support ",(0,r.jsx)(n.strong,{children:"event-driven workflows"})," and ",(0,r.jsx)(n.strong,{children:"automated DAG triggering"}),"."]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["This system is not limited to images \u2014 it can be adapted to trigger workflows from ",(0,r.jsx)(n.strong,{children:"any kind of file upload, sensor input, log event, or status change"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"core-use-case-pattern",children:"Core Use Case Pattern"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"User Uploads File/Trigger Event\r\n        \u2193\r\nFastAPI \u2192 Kafka Producer\r\n        \u2193\r\nKafka Topic (event ingestion)\r\n        \u2193\r\nAirflow DAG triggered via API\r\n        \u2193\r\nKafka Topic (result or signal)\r\n        \u2193\r\nFastAPI Kafka Consumer \u2192 Result returned/logged\r\n\n"})}),"\n",(0,r.jsx)(n.h3,{id:"common-use-cases-for-other-teams",children:"Common Use Cases for Other Teams"}),"\n",(0,r.jsx)(n.p,{children:"Here are examples of how teams across Redback can reuse this system without setting up Kafka/Airflow from scratch:"}),"\n",(0,r.jsx)(n.h4,{id:"data-cleaning-pipelines",children:"Data Cleaning Pipelines"}),"\n",(0,r.jsx)(n.p,{children:"Upload a data file through a Streamlit or FastAPI form \u2192 send to a Kafka topic \u2192 trigger Airflow DAG that:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Cleans null values"}),"\n",(0,r.jsx)(n.li,{children:"Validates schema"}),"\n",(0,r.jsx)(n.li,{children:"Pushes cleaned data to the Silver bucket in MinIO"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"ml-inference-on-upload",children:"ML Inference on Upload"}),"\n",(0,r.jsx)(n.p,{children:"Send image/audio/text files via Kafka \u2192 trigger Airflow DAG to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Run object detection, classification, or sentiment analysis"}),"\n",(0,r.jsx)(n.li,{children:"Store results back into MongoDB or Dremio"}),"\n",(0,r.jsx)(n.li,{children:"Notify user via FastAPI return or log entry"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"data-cataloging-or-metadata-extraction",children:"Data Cataloging or Metadata Extraction"}),"\n",(0,r.jsx)(n.p,{children:"Trigger a DAG when a new file is added to MinIO that:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Reads basic metadata (rows, columns, types)"}),"\n",(0,r.jsx)(n.li,{children:"Tags the file or stores metadata in PostgreSQL"}),"\n",(0,r.jsx)(n.li,{children:"Optionally emails or posts a summary to Teams/Slack"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"real-time-dashboard-updating",children:"Real-Time Dashboard Updating"}),"\n",(0,r.jsx)(n.p,{children:"Send sensor data or user entries via Kafka \u2192 DAG aggregates & stores latest stats \u2192 Dashboards pull new numbers without delay"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"how-to-reuse-this-pipeline",children:"How to Reuse This Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"You don\u2019t need to build a new Kafka setup. Just follow these steps:"}),"\n",(0,r.jsx)(n.h4,{id:"step-1-create-a-kafka-topic-optional",children:"Step 1: Create a Kafka Topic (Optional)"}),"\n",(0,r.jsxs)(n.p,{children:["Ask the DW team to help create a new topic (e.g., ",(0,r.jsx)(n.code,{children:"project5-cleaning-topic"}),") or reuse an existing one."]}),"\n",(0,r.jsx)(n.h4,{id:"step-2-write-your-airflow-dag",children:"Step 2: Write Your Airflow DAG"}),"\n",(0,r.jsx)(n.p,{children:"Your DAG should:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Listen for an incoming trigger (e.g., a filename, ID, or metadata)"}),"\n",(0,r.jsx)(n.li,{children:"Fetch the related file or data"}),"\n",(0,r.jsx)(n.li,{children:"Perform the required task (cleaning, prediction, merging, etc.)"}),"\n",(0,r.jsx)(n.li,{children:"Optionally publish back to Kafka or save to a storage system"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Example DAG trigger:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"@dag(schedule_interval=None)\r\ndef clean_csv_on_trigger():\r\n    # Pulls file info from Kafka or API\r\n    # Runs cleaning steps\r\n    # Stores clean file to MinIO Silver bucket\r\n\n"})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["documentation on this currently Work in progress. For now, this is the link of repository which contains the readme file and other information ",(0,r.jsx)(n.a,{href:"https://github.com/sumituiet/kafka_python/blob/main/README.md",children:"Kafka and airflow stack"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Sure! Here's the updated version of the ",(0,r.jsx)(n.strong,{children:"Data Warehouse Administration"})," section in clean Markdown:"]}),"\n",(0,r.jsx)(n.h2,{id:"administrative-services-overview",children:"Administrative Services Overview"}),"\n",(0,r.jsxs)(n.p,{children:["Beyond the core upload and query systems, the Redback Data Warehouse includes several ",(0,r.jsx)(n.strong,{children:"under-the-hood services"})," for tracking, recovery, and observability \u2014 mostly handled by the admin or DW lead."]}),"\n",(0,r.jsx)(n.h3,{id:"data-provenance-pipeline",children:"Data Provenance Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:["A provenance pipeline was introduced to ",(0,r.jsx)(n.strong,{children:"track system-level changes"})," across services \u2014 including uploads, deletions, access logs, and transformations."]}),"\n",(0,r.jsxs)(n.p,{children:["It uses the ",(0,r.jsx)(n.strong,{children:"ELK Stack"})," (Elasticsearch, Logstash, Kibana) with ",(0,r.jsx)(n.strong,{children:"PostgreSQL"})," as a central provenance store."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Elasticsearch"}),": Stores logs as JSON, searchable via:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"http://10.137.0.149:9200/minio-*/_search?pretty/",children:"MinIO Logs"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"http://10.137.0.149:9200/upload-service-*/_search?pretty/",children:"Upload Logs"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Logstash"}),": Parses logs and forwards them to Elasticsearch."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Kibana"}),": Optional frontend for dashboarding. Available at ",(0,r.jsx)(n.code,{children:"http://10.137.0.149:5601"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"PostgreSQL"}),": Long-term metadata store; access it using:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker exec -it postgres psql -U <username> -d <database-name>\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"Note: No dashboards have been set up in Kibana yet, but the stack is running and can be extended."}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://redback-operations.github.io/redback-documentation/docs/data-warehousing/Instructional%20Documents/Data%20Warehouse%20Overview",children:"click here to view detailed information on this"})}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"final-notes",children:"Final Notes"}),"\n",(0,r.jsxs)(n.p,{children:["This guide has aimed to provide a ",(0,r.jsx)(n.strong,{children:"complete and current overview"})," of the Redback Data Warehouse ecosystem \u2014 from data ingestion and processing to storage, querying, and backup. Whether you're a new contributor or a returning team member, this documentation should help you navigate the platform, understand the role of each component, and collaborate more effectively across teams."]}),"\n",(0,r.jsx)(n.p,{children:"As the platform evolves, so will this guide. If you notice missing pieces or have improvements to suggest, please contribute or notify the current Data Warehouse lead. Let\u2019s keep building better data infrastructure \u2014 together."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"version 1 - Document prepared by Daezel Goyal, Data Warehouse Leader \u2013 Redback Operations, May 2025"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var i=s(96540);const r={},t=i.createContext(r);function o(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);