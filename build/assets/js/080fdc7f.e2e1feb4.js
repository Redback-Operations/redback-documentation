"use strict";(self.webpackChunkredback_documentation=self.webpackChunkredback_documentation||[]).push([[9689],{40608:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"data-warehousing/Data preprocessing pipeline with MinIo/POC","title":"POC","description":"Last updated by \'09/05/2025\'","source":"@site/docs/data-warehousing/Data preprocessing pipeline with MinIo/POC.md","sourceDirName":"data-warehousing/Data preprocessing pipeline with MinIo","slug":"/data-warehousing/Data preprocessing pipeline with MinIo/POC","permalink":"/redback-documentation/docs/data-warehousing/Data preprocessing pipeline with MinIo/POC","draft":false,"unlisted":false,"editUrl":"https://github.com/Redback-Operations/redback-documentation/blob/main/docs/data-warehousing/Data preprocessing pipeline with MinIo/POC.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"sidebar_label":"MinIO Proof of Concept"},"sidebar":"tutorialSidebar","previous":{"title":"MinIO","permalink":"/redback-documentation/docs/category/minio"},"next":{"title":"MinIO Pipeline","permalink":"/redback-documentation/docs/data-warehousing/Data preprocessing pipeline with MinIo/Data Preprocessing Pipeline Doc"}}');var t=i(74848),r=i(28453);const o={sidebar_position:1,sidebar_label:"MinIO Proof of Concept"},l="Proof of Concept: Data Preprocessing Pipeline with MinIO",a={},d=[{value:"Project Overview",id:"project-overview",level:2},{value:"Objectives",id:"objectives",level:2},{value:"Components Involved",id:"components-involved",level:2},{value:"Steps Demonstrated in PoC",id:"steps-demonstrated-in-poc",level:2},{value:"Challenges Identified",id:"challenges-identified",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={admonition:"admonition",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Last updated by:"})," RichardWhellum, ",(0,t.jsx)(n.strong,{children:"Last updated on:"})," '09/05/2025'"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Last updated by:"})," RichardWhellum, ",(0,t.jsx)(n.strong,{children:"Last updated on:"})," '09/05/2025'"]}),"\n",(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"proof-of-concept-data-preprocessing-pipeline-with-minio",children:"Proof of Concept: Data Preprocessing Pipeline with MinIO"})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Document Creation:"})," 22 September, 2024. ",(0,t.jsx)(n.strong,{children:"Last Edited:"})," 22 September, 2024. ",(0,t.jsx)(n.strong,{children:"Authors:"})," Meghana Kaveti.\r\n",(0,t.jsx)("br",{})," ",(0,t.jsx)(n.strong,{children:"Document Code:"})," MIN1. ",(0,t.jsx)(n.strong,{children:"Effective Date:"})," 22 September 2024. ",(0,t.jsx)(n.strong,{children:"Expiry Date:"})," 5 September 2025."]})}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsx)(n.p,{children:"The goal of this Proof of Concept is to demonstrate the feasibility and functionality of a data\r\npipeline that processes raw CSV files stored in MinIO buckets, performs data\r\ncleaning/preprocessing, and uploads the processed data to another MinIO bucket for further\r\nanalysis. The system is designed to efficiently handle CSV files, address data quality issues, and\r\nprovide a seamless transition from raw data (Bronze bucket) to processed, clean data (Silver\r\nbucket)."}),"\n",(0,t.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verify MinIO Connectivity"}),": Ensure the MinIO client can successfully connect to the\r\nserver using provided credentials and list available buckets."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Download Files from Source Buckets"}),": Retrieve CSV files from designated project\r\nbuckets (project-2, project-3) and transfer them to the dw-bucket-bronze bucket for raw\r\nstorage."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Preprocessing"}),": Implement preprocessing steps to clean the raw data by removing\r\nempty columns, filling missing values, and standardizing the structure of the dataset."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Upload Preprocessed Files to Silver Bucket"}),": Upload the cleaned CSV files to the dw\r\n-bucket-silver bucket for further analysis."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automation"}),": Provide the capability to process multiple files and automate the workflow\r\nfor future datasets."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"components-involved",children:"Components Involved"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MinIO"}),": An object storage solution used to store raw and processed data in structured\r\nbuckets."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MinIO Python Client (minio SDK)"}),": A Python library used to interact with MinIO, manage\r\nfiles, and move data between buckets."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pandas"}),": A Python library used for data processing, including handling missing values,\r\nremoving empty columns, and transforming data."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Logging"}),": Python\u2019s logging module is used to track processing progress, errors, and\r\ncompletion."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"steps-demonstrated-in-poc",children:"Steps Demonstrated in PoC"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"MinIO Client Setup"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The MinIO client was successfully initialized with environment variables for the host, access\r\nkey, and secret key. A function was developed to list all available buckets in MinIO, ensuring that\r\nthe connection is active and functioning.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verification"}),": Listing the available buckets confirmed connectivity to the MinIO server."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"File Handling and Transfer"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The next step was to download CSV files from the project-2 and project-3 buckets into the dw\r\n-bucket-bronze bucket. A naming convention was applied to each file, ensuring consistency in\r\nfile naming based on the project name, dataset description, and a timestamp.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verification"}),": Files were successfully transferred from the source buckets to the Bronze\r\nbucket with the correct naming convention."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Data Preprocessing"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["CSV files from the Bronze bucket were pre-processed by:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Removing Empty Columns"}),": Columns that had no data were automatically dropped."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handling Missing Values"}),": Numeric columns with missing values were filled with the\r\ncolumn\u2019s median value."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Header Validation"}),": For files with potential missing headers, the first row was used as a\r\nheader, and preprocessing was adjusted accordingly."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verification"}),": Preprocessing successfully cleaned the data, ensuring it was ready for\r\nfurther use."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"File Upload to Silver Bucket"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The processed files were then uploaded to the dw-bucket-silver bucket for storage. This\r\nseparates the raw, uncleaned data (Bronze) from the cleaned and ready-to-use data (Silver).","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verification"}),": Processed files were successfully uploaded to the Silver bucket with the\r\ncorrect file structure."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Error Handling and Logging"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The PoC includes robust error handling for common issues, such as failed file downloads or\r\ninvalid CSV structures. Logging is used throughout to track progress, flag errors, and confirm\r\nsuccessful uploads.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verification"}),": The system successfully logged key actions, and any issues during file\r\ntransfer or processing were handled with appropriate warnings or errors."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"challenges-identified",children:"Challenges Identified"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handling files without headers"})," required additional validation and preprocessing logic."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Some datasets required more complex handling of missing values"}),", which could lead to\r\nadditional customization in the pipeline."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Testing at Scale"}),": Validate the system\u2019s functionality with larger datasets and more files\r\nto ensure that the pipeline can handle high-volume operations efficiently."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automation"}),": Automate the process to continuously monitor the source buckets and\r\nautomatically trigger preprocessing and uploads when new files are added."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Security Enhancements"}),": Implement access controls to ensure that only authorized\r\nusers can upload or modify files in the buckets."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"The Proof of Concept successfully demonstrates that the data preprocessing pipeline can be\r\nimplemented in MinIO using Python. CSV files can be downloaded, cleaned, and moved\r\nbetween buckets efficiently, with clear logging and error handling in place. The pipeline is ready\r\nfor scaling, automation, and integration into broader data workflows at Redback Operations."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var s=i(96540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);